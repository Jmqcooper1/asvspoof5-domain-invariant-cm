<!DOCTYPE html>
<!--
  Domain-Invariant Speech Deepfake Detection - Thesis Presentation
  
  NOTE: This presentation requires an internet connection to load:
  - Reveal.js framework from cdn.jsdelivr.net
  - Google Fonts (Source Serif 4, Source Sans 3, JetBrains Mono)
  
  See README.md for more details.
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain-Invariant Speech Deepfake Detection</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/white.css">
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@400;600;700&family=Source+Sans+3:wght@400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --heading-font: 'Source Serif 4', Georgia, serif;
            --body-font: 'Source Sans 3', -apple-system, sans-serif;
            --mono-font: 'JetBrains Mono', monospace;
            --accent: #1e40af;
            --accent-light: #dbeafe;
            --text-primary: #1e293b;
            --text-secondary: #475569;
            --border: #e2e8f0;
            --surface: #f8fafc;
        }
        
        .reveal {
            font-family: var(--body-font);
            font-size: 24px;
            color: var(--text-primary);
        }
        
        .reveal h1, .reveal h2, .reveal h3 {
            font-family: var(--heading-font);
            font-weight: 600;
            color: var(--text-primary);
            text-transform: none;
            letter-spacing: -0.01em;
            line-height: 1.2;
        }
        
        .reveal h1 { font-size: 2em; margin-bottom: 0.3em; }
        .reveal h2 { font-size: 1.5em; margin-bottom: 0.5em; }
        .reveal h3 { font-size: 1.1em; margin-bottom: 0.4em; color: var(--text-secondary); }
        
        .reveal section {
            text-align: left;
            height: 100%;
            padding: 1em 2em;
            box-sizing: border-box;
        }
        
        .reveal .center { text-align: center; }
        
        .reveal ul, .reveal ol {
            margin: 0.5em 0 0.5em 1.2em;
            line-height: 1.5;
        }
        
        .reveal li {
            margin: 0.25em 0;
            font-size: 0.9em;
        }
        
        .reveal li li { font-size: 0.95em; }
        
        .accent { color: var(--accent); }
        .muted { color: var(--text-secondary); }
        .small { font-size: 0.75em; color: var(--text-secondary); }
        
        .two-col {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2em;
            align-items: start;
        }
        
        .col { min-height: 0; }
        
        table.data {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.8em;
            margin: 0.5em 0;
        }
        
        table.data th, table.data td {
            border: 1px solid var(--border);
            padding: 0.4em 0.6em;
            text-align: left;
        }
        
        table.data th {
            background: var(--surface);
            font-weight: 600;
        }
        
        .card {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 6px;
            padding: 0.6em 1em;
            margin: 0.4em 0;
        }
        
        .card-accent {
            background: var(--accent-light);
            border-left: 3px solid var(--accent);
        }
        
        .formula {
            font-family: var(--mono-font);
            font-size: 0.85em;
            background: var(--surface);
            padding: 0.3em 0.6em;
            border-radius: 4px;
            display: inline-block;
        }
        
        .tag {
            display: inline-block;
            font-size: 0.7em;
            font-weight: 600;
            padding: 0.15em 0.5em;
            border-radius: 3px;
            vertical-align: middle;
        }
        
        .tag-green { background: #dcfce7; color: #166534; }
        .tag-amber { background: #fef3c7; color: #92400e; }
        .tag-blue { background: #dbeafe; color: #1e40af; }
        
        .status-pending {
            background: #fffbeb;
            border: 1px dashed #f59e0b;
            border-radius: 6px;
            padding: 1em;
            text-align: center;
            color: #92400e;
        }
        
        .checklist {
            list-style: none;
            margin-left: 0;
            padding-left: 0;
        }
        
        .checklist li {
            padding-left: 1.5em;
            position: relative;
        }
        
        .checklist li::before {
            position: absolute;
            left: 0;
            font-weight: 600;
        }
        
        .checklist li.done::before { content: "✓"; color: #16a34a; }
        .checklist li.pending::before { content: "○"; color: #f59e0b; }
        .checklist li.future::before { content: "·"; color: #94a3b8; }
        
        .reveal .slides section { overflow: hidden; }

        .flow-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.3em;
            margin: 0.5em auto;
        }
        
        .flow-box {
            padding: 0.4em 1em;
            border-radius: 6px;
            font-size: 0.75em;
            font-weight: 500;
            text-align: center;
            min-width: 180px;
        }
        
        .flow-box.blue { background: #dbeafe; border: 2px solid #3b82f6; }
        .flow-box.green { background: #dcfce7; border: 2px solid #22c55e; }
        .flow-box.orange { background: #fed7aa; border: 2px solid #f97316; }
        .flow-box.purple { background: #e9d5ff; border: 2px solid #a855f7; }
        .flow-box.gray { background: #f1f5f9; border: 2px solid #94a3b8; }
        
        .flow-arrow { color: #64748b; font-size: 1.2em; }
        
        .flow-split {
            display: flex;
            gap: 2em;
            justify-content: center;
        }
        
        .flow-branch {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.3em;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- 1. Title -->
            <section class="center" style="display: flex; flex-direction: column; justify-content: center;">
                <h1 style="font-size: 2.2em;">Domain-Invariant Speech<br>Deepfake Detection</h1>
                <h3 style="font-weight: 400; margin-top: 0.5em;">Domain-Adversarial Training on ASVspoof 5</h3>
                <p style="margin-top: 2.5em; color: var(--text-secondary);">
                    Mike Cooper<br>
                    <span class="small">Supervisor Meeting · February 2026</span>
                </p>
            </section>
            
            <!-- 2. Problem Statement -->
            <section>
                <h2>The Problem: Domain Shift</h2>
                <div class="two-col" style="margin-top: 0.5em;">
                    <div class="col">
                        <h3>Detectors Fail Under Real Conditions</h3>
                        <ul>
                            <li>Models trained on clean data <span class="accent">degrade</span> when audio is compressed, transmitted, or re-recorded</li>
                            <li>Detectors learn <strong>shortcut cues</strong> tied to codec artifacts</li>
                            <li>Attackers can exploit this mismatch</li>
                        </ul>
                    </div>
                    <div class="col">
                        <h3>Why It Matters</h3>
                        <ul>
                            <li>Real-world audio is never pristine</li>
                            <li>Messaging apps, VoIP, and social media all apply compression</li>
                        </ul>
                        <div class="card card-accent" style="margin-top: 0.8em;">
                            <strong>Goal:</strong> Build detectors robust to codec and channel shifts
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- 3. ASVspoof 5 Dataset -->
            <section>
                <h2>ASVspoof 5: A Modern Benchmark</h2>
                <ul style="margin-bottom: 0.5em;">
                    <li><strong>Track 1:</strong> Stand-alone bonafide vs spoof classification</li>
                    <li><strong>Crowdsourced audio</strong> with diverse recording conditions</li>
                    <li><strong>11 codec configurations</strong> (C01–C11): OPUS, AMR, Speex, Encodec, MP3, AAC, device channels</li>
                </ul>
                <table class="data">
                    <tr>
                        <th>Split</th>
                        <th>Samples</th>
                        <th>Codec Diversity</th>
                    </tr>
                    <tr>
                        <td>Train</td>
                        <td>~180k</td>
                        <td>Uncoded only</td>
                    </tr>
                    <tr>
                        <td>Dev</td>
                        <td>~140k</td>
                        <td>Uncoded only</td>
                    </tr>
                    <tr>
                        <td>Eval</td>
                        <td>~700k</td>
                        <td><strong>Full codec diversity</strong></td>
                    </tr>
                </table>
                <p class="small" style="margin-top: 0.3em;">Train/Dev have no codec shift — the evaluation set is the true generalization test.</p>
            </section>
            
            <!-- 4. Research Questions & Hypotheses -->
            <section>
                <h2>Research Questions & Hypotheses</h2>
                <div class="two-col">
                    <div class="col">
                        <h3>Research Questions</h3>
                        <ul style="font-size: 0.85em;">
                            <li><strong>RQ1:</strong> Does DANN improve generalization under codec shift vs ERM?</li>
                            <li><strong>RQ2:</strong> What is the trade-off between detection and domain invariance?</li>
                            <li><strong>RQ3:</strong> Where is codec information encoded, and how does DANN change it?</li>
                            <li><strong>RQ4:</strong> Can activation patching reduce domain leakage without retraining?</li>
                        </ul>
                    </div>
                    <div class="col">
                        <h3>Hypotheses</h3>
                        <ul style="font-size: 0.85em;">
                            <li><strong>H1:</strong> DANN reduces domain leakage and improves out-of-domain EER</li>
                            <li><strong>H2:</strong> Domain information concentrates in specific layers; DANN suppresses it</li>
                            <li><strong>H3:</strong> Effects are consistent across WavLM and Wav2Vec2, but layer-wise localization differs</li>
                        </ul>
                    </div>
                </div>
            </section>
            
            <!-- 5. ASVspoof5 Design: Addressing Shortcut Learning -->
            <section>
                <h2>ASVspoof5 Design: Addressing Shortcut Learning</h2>
                <div class="card card-accent" style="margin: 0.5em 0; font-style: italic; font-size: 0.85em;">
                    "We have been alerted to the presence of shortcut artefacts which plagues some earlier ASVspoof databases — namely artefacts which are semantically unrelated to the spoof/deepfake problem but which can nonetheless be utilised for detection."
                    <p class="small" style="margin-top: 0.3em; font-style: normal;">— ASVspoof5 Paper</p>
                </div>
                <div class="two-col" style="margin-top: 0.6em;">
                    <div class="col">
                        <h3>The Deliberate Design Choice</h3>
                        <table class="data">
                            <tr>
                                <th>Split</th>
                                <th>Codec Diversity</th>
                            </tr>
                            <tr>
                                <td>Train</td>
                                <td>Clean only (no codecs)</td>
                            </tr>
                            <tr>
                                <td>Dev</td>
                                <td>Clean only (no codecs)</td>
                            </tr>
                            <tr>
                                <td><strong>Eval</strong></td>
                                <td><strong>C01–C11</strong> (full diversity)</td>
                            </tr>
                        </table>
                        <p class="small" style="margin-top: 0.3em;">This forces models to generalize to unseen transmission conditions.</p>
                    </div>
                    <div class="col">
                        <h3>Our Solution: DANN + Synthetic Augmentation</h3>
                        <ul style="font-size: 0.85em;">
                            <li><strong>Problem:</strong> No codec labels in train/dev to learn from</li>
                            <li><strong>Solution:</strong> Create synthetic codec domains via FFmpeg</li>
                            <li><strong>Goal:</strong> Learn <span class="accent">domain-invariant features</span> that generalize to real eval codecs</li>
                        </ul>
                        <div class="card" style="margin-top: 0.6em; font-size: 0.85em;">
                            <strong>This is exactly what the dataset designers hoped for</strong> — systems that detect deepfakes based on spoofing artifacts, not codec shortcuts.
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- 5b. Synthetic Augmentation Coverage Analysis -->
            <section>
                <h2>Synthetic Augmentation Coverage</h2>
                <p>We analyzed the ASVspoof5 protocol files to understand what our synthetic augmentation can and cannot cover:</p>
                <div class="two-col" style="margin-top: 0.5em;">
                    <div class="col">
                        <h3>Dataset Composition</h3>
                        <table class="data">
                            <tr>
                                <th>Split</th>
                                <th>Samples</th>
                                <th>Codec Diversity</th>
                            </tr>
                            <tr>
                                <td>Train</td>
                                <td>182,357</td>
                                <td>100% NONE</td>
                            </tr>
                            <tr>
                                <td>Dev</td>
                                <td>140,950</td>
                                <td>100% NONE</td>
                            </tr>
                            <tr>
                                <td><strong>Eval</strong></td>
                                <td><strong>680,774</strong></td>
                                <td><strong>12 conditions</strong></td>
                            </tr>
                        </table>
                        <div class="card card-accent" style="margin-top: 0.5em; font-size: 0.85em;">
                            <strong>79.2%</strong> of eval covered by synthetic augmentation<br>
                            <strong>20.8%</strong> uncovered (C04, C07, C11)
                        </div>
                    </div>
                    <div class="col">
                        <h3>Eval Set Codec Distribution</h3>
                        <table class="data" style="font-size: 0.75em;">
                            <tr><th>Codec</th><th>%</th><th>Synthetic</th></tr>
                            <tr><td>NONE</td><td>25.2%</td><td>✓</td></tr>
                            <tr><td>C01 (OPUS)</td><td>6.9%</td><td>✓</td></tr>
                            <tr><td>C02 (AMR)</td><td>7.0%</td><td>✓</td></tr>
                            <tr><td>C03 (SPEEX)</td><td>7.0%</td><td>✓</td></tr>
                            <tr style="background: #fef2f2;"><td>C04 (Encodec)</td><td>7.0%</td><td>❌</td></tr>
                            <tr><td>C05 (MP3)</td><td>7.0%</td><td>✓</td></tr>
                            <tr><td>C06 (AAC)</td><td>6.1%</td><td>✓</td></tr>
                            <tr style="background: #fef2f2;"><td>C07 (Cascade)</td><td>7.0%</td><td>❌</td></tr>
                            <tr><td>C08 (OPUS-NB)</td><td>6.9%</td><td>✓</td></tr>
                            <tr><td>C09 (AMR-NB)</td><td>6.1%</td><td>✓</td></tr>
                            <tr><td>C10 (SPEEX-NB)</td><td>7.0%</td><td>✓</td></tr>
                            <tr style="background: #fef2f2;"><td>C11 (Device)</td><td>6.8%</td><td>❌</td></tr>
                        </table>
                    </div>
                </div>
                <p class="small" style="margin-top: 0.3em;"><strong>Uncovered codecs:</strong> C04 (neural codec), C07 (MP3+Encodec cascade), C11 (real device/telephony effects) — these remain inherently challenging.</p>
            </section>
            
            <!-- 6. Multi-Task Domain Adaptation: CODEC & CODEC_Q -->
            <section>
                <h2>Multi-Task Domain Adaptation: CODEC & CODEC_Q</h2>
                <p>We define <strong>two independent domain discrimination tasks</strong> to cover the full space of transmission conditions:</p>
                <div class="two-col" style="margin-top: 0.5em;">
                    <div class="col">
                        <div class="card" style="border-left: 3px solid #a855f7;">
                            <h3 style="margin-top: 0;">CODEC <span class="tag tag-blue">Format Type</span></h3>
                            <p style="font-size: 0.85em;">The compression format used:</p>
                            <ul style="font-size: 0.8em;">
                                <li>MP3, AAC (lossy audio)</li>
                                <li>OPUS, Speex (VoIP)</li>
                                <li>AMR (telephony)</li>
                                <li>Encodec (neural codec)</li>
                            </ul>
                            <p class="small" style="margin-top: 0.4em;"><strong>Why it matters:</strong> Different codecs introduce <em>different types</em> of artifacts (psychoacoustic vs. neural vs. speech-specific)</p>
                        </div>
                    </div>
                    <div class="col">
                        <div class="card" style="border-left: 3px solid #f97316;">
                            <h3 style="margin-top: 0;">CODEC_Q <span class="tag tag-amber">Quality Level</span></h3>
                            <p style="font-size: 0.85em;">Bitrate/quality setting (1–5):</p>
                            <ul style="font-size: 0.8em;">
                                <li><strong>1:</strong> Heavy compression (low bitrate)</li>
                                <li><strong>3:</strong> Medium quality</li>
                                <li><strong>5:</strong> Light compression (high bitrate)</li>
                            </ul>
                            <p class="small" style="margin-top: 0.4em;"><strong>Why it matters:</strong> Different quality levels = different <em>severity</em> of artifacts</p>
                        </div>
                    </div>
                </div>
                <div class="card card-accent" style="margin-top: 0.6em;">
                    <strong>Key insight:</strong> CODEC and CODEC_Q are <strong>independent</strong> — any codec can have any quality level. By adapting to both jointly, the model becomes robust across the full space of real-world transmission conditions.
                </div>
            </section>
            
            <!-- 7. Method Overview -->
            <section>
                <h2>Method: ERM vs DANN</h2>
                <div class="two-col">
                    <div class="col">
                        <h3>ERM <span class="tag tag-blue">Baseline</span></h3>
                        <ul>
                            <li>Standard supervised training</li>
                            <li>Minimize task loss only</li>
                            <li>No domain awareness</li>
                        </ul>
                        <p style="margin-top: 0.8em;"><span class="formula">L = L<sub>task</sub></span></p>
                    </div>
                    <div class="col">
                        <h3>DANN <span class="tag tag-green">Proposed</span></h3>
                        <ul>
                            <li>Domain-Adversarial Neural Network</li>
                            <li>Gradient Reversal Layer (GRL)</li>
                            <li>Forces domain-invariant features</li>
                        </ul>
                        <p style="margin-top: 0.8em;"><span class="formula">L = L<sub>task</sub> + λ · (L<sub>codec</sub> + L<sub>codec_q</sub>)</span></p>
                    </div>
                </div>
                <div class="card" style="margin-top: 0.8em;">
                    <strong>Key insight:</strong> DANN learns representations that discriminate deepfakes while remaining invariant to codec domains.
                </div>
            </section>
            
            <!-- 8. Architecture Diagram -->
            <section>
                <h2>Model Architecture</h2>
                <div class="flow-diagram">
                    <div class="flow-box gray">Audio Waveform</div>
                    <div class="flow-arrow">↓</div>
                    <div class="flow-box blue">SSL Backbone (WavLM / Wav2Vec2)<br><span class="small" style="color: #1e40af;">FROZEN</span></div>
                    <div class="flow-arrow">↓</div>
                    <div class="flow-box green">Layer Mixing + Projection Head</div>
                    <div class="flow-arrow">↓</div>
                    <div class="flow-split">
                        <div class="flow-branch">
                            <div class="flow-box orange">Task Classifier</div>
                            <div class="flow-arrow">↓</div>
                            <div class="small"><strong>bonafide / spoof</strong></div>
                        </div>
                        <div class="flow-branch">
                            <div class="flow-box purple">GRL (λ)</div>
                            <div class="flow-arrow">↓</div>
                            <div class="flow-box purple">Domain Discriminator</div>
                            <div class="flow-arrow">↓</div>
                            <div class="small"><strong>CODEC / CODEC_Q</strong></div>
                        </div>
                    </div>
                </div>
                <p class="small center" style="margin-top: 0.3em;">GRL = Gradient Reversal Layer: identity on forward pass, negates gradients on backward pass</p>
            </section>
            
            <!-- 9. Gradient Reversal -->
            <section>
                <h2>Gradient Reversal Layer</h2>
                <div class="two-col">
                    <div class="col">
                        <h3>Forward Pass</h3>
                        <div class="formula">GRL(x) = x</div>
                        <p class="small" style="margin-top: 0.3em;">Identity — discriminator sees normal features</p>
                    </div>
                    <div class="col">
                        <h3>Backward Pass</h3>
                        <div class="formula">∂GRL/∂x = −λ · grad</div>
                        <p class="small" style="margin-top: 0.3em;">Reversed gradients push backbone to confuse discriminator</p>
                    </div>
                </div>
                <div class="card card-accent" style="margin-top: 1em;">
                    <strong>Effect:</strong> The backbone learns to produce representations where codec information is suppressed, while task-relevant signal is preserved.
                </div>
                <h3 style="margin-top: 0.8em;">Lambda Schedule</h3>
                <ul>
                    <li>Exponential warmup: λ starts small, grows during training</li>
                    <li>Prevents early destabilization of the adversarial game</li>
                </ul>
            </section>
            
            <!-- 10. Multi-Head Discriminator -->
            <section>
                <h2>Multi-Head Domain Discriminator</h2>
                <div class="flow-diagram" style="margin-top: 1em;">
                    <div class="flow-box green">Shared Features (256-dim)</div>
                    <div class="flow-arrow">↓</div>
                    <div class="flow-box gray">Shared MLP</div>
                    <div class="flow-arrow">↓</div>
                    <div class="flow-split">
                        <div class="flow-box purple">CODEC Head<br><span class="small" style="color: #7c3aed;">N classes</span></div>
                        <div class="flow-box purple">CODEC_Q Head<br><span class="small" style="color: #7c3aed;">M classes</span></div>
                    </div>
                </div>
                <ul style="margin-top: 1em;">
                    <li><strong>Two domain tasks:</strong> predict codec type and codec quality separately</li>
                    <li><strong>Synthetic augmentation:</strong> apply MP3/AAC/OPUS compression during training</li>
                    <li>Encourages invariance to both codec family and quality level</li>
                </ul>
            </section>
            
            <!-- 11. Backbones & Baselines -->
            <section>
                <h2>Backbones & Baselines</h2>
                <h3>SSL Backbones (Frozen)</h3>
                <table class="data">
                    <tr>
                        <th>Backbone</th>
                        <th>Layers</th>
                        <th>Hidden Dim</th>
                        <th>Pre-training</th>
                    </tr>
                    <tr>
                        <td><strong>WavLM Base+</strong></td>
                        <td>12</td>
                        <td>768</td>
                        <td>94k hours (diverse)</td>
                    </tr>
                    <tr>
                        <td><strong>Wav2Vec 2.0 Base</strong></td>
                        <td>12</td>
                        <td>768</td>
                        <td>960h (LibriSpeech)</td>
                    </tr>
                </table>
                <h3 style="margin-top: 0.6em;">Baselines</h3>
                <ul>
                    <li><strong>ERM:</strong> Same architecture without domain adversary (per backbone)</li>
                    <li><strong>TRILLsson:</strong> Paralinguistic embeddings + classifier (non-semantic)</li>
                    <li><strong>LFCC-GMM:</strong> Classical acoustic features + Gaussian Mixture Model</li>
                    <li><strong>AASIST / RawNet:</strong> Official ASVspoof baselines (reference)</li>
                </ul>
            </section>
            
            <!-- 12. Codec Augmentation -->
            <section>
                <h2>Synthetic Codec Augmentation</h2>
                <p>Train/Dev have no codec diversity — we create it synthetically via FFmpeg:</p>
                <table class="data" style="margin-top: 0.4em;">
                    <tr>
                        <th>Synthetic</th>
                        <th>ASVspoof 5 Codecs</th>
                        <th>Status</th>
                    </tr>
                    <tr>
                        <td>NONE</td>
                        <td>"−" (original)</td>
                        <td><span class="tag tag-green">Simulated</span></td>
                    </tr>
                    <tr>
                        <td>MP3, AAC</td>
                        <td>C05, C06</td>
                        <td><span class="tag tag-green">Simulated</span></td>
                    </tr>
                    <tr>
                        <td>OPUS, SPEEX, AMR</td>
                        <td>C01, C02, C03, C08, C09, C10</td>
                        <td><span class="tag tag-green">Simulated</span></td>
                    </tr>
                    <tr>
                        <td>—</td>
                        <td>C04 (Encodec), C07 (cascade), C11 (device)</td>
                        <td><span class="tag tag-amber">Not simulated</span></td>
                    </tr>
                </table>
                <p class="small" style="margin-top: 0.4em;">Augmentations are pre-computed offline for ~5× training speedup vs on-the-fly FFmpeg.</p>
            </section>
            
            <!-- 13. Training Setup -->
            <section>
                <h2>Training Setup</h2>
                <div class="two-col">
                    <div class="col">
                        <h3>Hyperparameters</h3>
                        <ul>
                            <li>Optimizer: AdamW, lr = 1e-4</li>
                            <li>Batch size: 128</li>
                            <li>Max epochs: 50 (early stopping)</li>
                            <li>λ schedule: exponential warmup</li>
                        </ul>
                    </div>
                    <div class="col">
                        <h3>Infrastructure</h3>
                        <ul>
                            <li>Snellius HPC (SURF)</li>
                            <li>GPU: A100 (48GB)</li>
                            <li>~24–48h per DANN run</li>
                            <li>W&B tracking</li>
                        </ul>
                    </div>
                </div>
                <h3 style="margin-top: 0.6em;">Recent Fixes</h3>
                <ul class="checklist">
                    <li class="done">Pre-compute codec augmentations offline (PR #42, #46, #47)</li>
                    <li class="done">Fix discriminator double-λ bug (PR #44)</li>
                    <li class="done">Exponential λ schedule + pre-projection tap (PR #44)</li>
                    <li class="done">Domain probe diagnostic script (PR #41)</li>
                </ul>
            </section>
            
            <!-- 13b. Lambda Instability & Tuning -->
            <section>
                <h2>Lambda Instability & Tuning</h2>
                <p>DANN training is inherently unstable — the adversarial game can destabilize if λ grows too fast or too high.</p>
                <div class="two-col" style="margin-top: 0.5em;">
                    <div class="col">
                        <h3>Problem Discovered</h3>
                        <div class="card" style="background: #fef2f2; border-left: 3px solid #ef4444;">
                            <strong>λ &gt; 0.75 → gradient explosion</strong>
                            <ul style="font-size: 0.8em; margin-top: 0.3em;">
                                <li>Task loss spikes suddenly</li>
                                <li>Discriminator wins too decisively</li>
                                <li>Backbone features collapse</li>
                            </ul>
                        </div>
                        <p class="small" style="margin-top: 0.4em;">Observed in WavLM DANN runs when using exponential λ warmup to 1.0</p>
                    </div>
                    <div class="col">
                        <h3>Solution (PR #62)</h3>
                        <div class="card" style="background: #f0fdf4; border-left: 3px solid #22c55e;">
                            <strong>Stabilized λ schedule:</strong>
                            <ul style="font-size: 0.8em; margin-top: 0.3em;">
                                <li><strong>Linear</strong> warmup (not exponential)</li>
                                <li>λ: 0.1 → 0.75 (capped)</li>
                                <li>3 warmup epochs before λ starts</li>
                                <li>Gradient clipping: max_norm = 0.5</li>
                            </ul>
                        </div>
                        <p class="small" style="margin-top: 0.4em;">Result: Stable training for 20+ epochs, best checkpoints at epoch 6 (WavLM) and 14 (Wav2Vec2)</p>
                    </div>
                </div>
                <div class="card card-accent" style="margin-top: 0.6em;">
                    <strong>Takeaway:</strong> DANN hyperparameter tuning is critical. The adversarial strength (λ) must be carefully balanced — too low and domain info leaks through, too high and the model collapses.
                </div>
            </section>
            
            <!-- 13c. Challenges & Solutions -->
            <section>
                <h2>Challenges & Solutions</h2>
                <p>Key engineering contributions during implementation:</p>
                <table class="data" style="margin-top: 0.5em;">
                    <tr>
                        <th>Challenge</th>
                        <th>Impact</th>
                        <th>Solution</th>
                    </tr>
                    <tr>
                        <td><strong>5× training slowdown</strong></td>
                        <td>On-the-fly FFmpeg codec conversion bottlenecked I/O</td>
                        <td>Pre-cached augmentation: 820k AAC + 820k MP3 files (PR #42, #46)</td>
                    </tr>
                    <tr>
                        <td><strong>Double-λ bug</strong></td>
                        <td>GRL applied λ twice — once in GRL, once in loss weighting</td>
                        <td>Fixed GRL to use λ=1, apply scaling only in loss (PR #44)</td>
                    </tr>
                    <tr>
                        <td><strong>λ instability</strong></td>
                        <td>Gradient explosion when λ &gt; 0.75</td>
                        <td>Linear schedule, capped λ, gradient clipping (PR #62)</td>
                    </tr>
                    <tr>
                        <td><strong>Discriminator tap point</strong></td>
                        <td>Post-projection features lost domain signal</td>
                        <td>Moved discriminator tap to pre-projection (PR #44)</td>
                    </tr>
                </table>
                <div class="card card-accent" style="margin-top: 0.6em;">
                    <strong>Lesson:</strong> Domain-adversarial training requires careful engineering. The original DANN paper glosses over many practical details that matter significantly at scale.
                </div>
            </section>
            
            <!-- 14. Evaluation Metrics -->
            <section>
                <h2>Evaluation Metrics</h2>
                <p>We evaluate using three complementary metrics from speaker verification and anti-spoofing:</p>
                <div class="two-col" style="margin-top: 0.5em;">
                    <div class="col">
                        <div class="card" style="border-left: 3px solid #3b82f6;">
                            <h3 style="margin-top: 0;">EER <span class="tag tag-blue">Primary</span></h3>
                            <p style="font-size: 0.85em;"><strong>Equal Error Rate</strong></p>
                            <ul style="font-size: 0.8em;">
                                <li>Point where FAR = FRR</li>
                                <li>Threshold-independent single value</li>
                                <li>Standard metric in speaker verification</li>
                            </ul>
                            <p class="small" style="margin-top: 0.4em;"><strong>Why:</strong> Easy to interpret, widely reported, enables fair comparison across systems</p>
                        </div>
                        <div class="card" style="border-left: 3px solid #22c55e; margin-top: 0.6em;">
                            <h3 style="margin-top: 0;">minDCF <span class="tag tag-green">Application</span></h3>
                            <p style="font-size: 0.85em;"><strong>Minimum Detection Cost Function</strong></p>
                            <ul style="font-size: 0.8em;">
                                <li>Weighs misses vs false alarms</li>
                                <li>Reflects real deployment costs</li>
                                <li>Prior-weighted: P<sub>target</sub> = 0.05</li>
                            </ul>
                            <p class="small" style="margin-top: 0.4em;"><strong>Why:</strong> More realistic for practical systems where error costs differ</p>
                        </div>
                    </div>
                    <div class="col">
                        <div class="card" style="border-left: 3px solid #a855f7;">
                            <h3 style="margin-top: 0;">t-DCF <span class="tag tag-amber">Tandem</span></h3>
                            <p style="font-size: 0.85em;"><strong>Tandem Detection Cost Function</strong></p>
                            <ul style="font-size: 0.8em;">
                                <li>ASVspoof-specific metric</li>
                                <li>Considers CM + ASV as a tandem system</li>
                                <li>Captures spoofs that would fool the full pipeline</li>
                            </ul>
                            <p class="small" style="margin-top: 0.4em;"><strong>Why:</strong> Measures real-world impact — a spoof only matters if it fools both the countermeasure AND the speaker verifier</p>
                        </div>
                        <div class="card card-accent" style="margin-top: 0.6em;">
                            <strong>Key insight:</strong> EER tells us detection quality, minDCF adds deployment context, t-DCF reveals actual security impact on speaker verification.
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- 15. Results -->
            <section>
                <h2>Results</h2>
                <div class="card" style="background: #fefce8; border: 1px solid #facc15;">
                    <strong>Training Status:</strong> DANN runs completed with stabilized λ schedule<br>
                    <span class="small">Best checkpoints: WavLM DANN (epoch 6), Wav2Vec2 DANN (epoch 14)</span>
                </div>
                <h3 style="margin-top: 0.8em;">Evaluation Plan</h3>
                <table class="data">
                    <tr>
                        <th>Metric</th>
                        <th>WavLM ERM</th>
                        <th>WavLM DANN</th>
                        <th>W2V2 ERM</th>
                        <th>W2V2 DANN</th>
                    </tr>
                    <tr>
                        <td>EER (overall)</td>
                        <td>~4%*</td>
                        <td>Pending eval</td>
                        <td>~4%*</td>
                        <td>Pending eval</td>
                    </tr>
                    <tr>
                        <td>minDCF</td>
                        <td>TBD</td>
                        <td>TBD</td>
                        <td>TBD</td>
                        <td>TBD</td>
                    </tr>
                    <tr>
                        <td>Per-codec EER</td>
                        <td colspan="4" class="muted"><em>Key test for H1 — improvement on worst codecs</em></td>
                    </tr>
                </table>
                <p class="small">*Preliminary ERM results from Wave 5; models overfit from epoch 1. Full eval on 680k samples pending.</p>
            </section>
            
            <!-- 16. Domain Probe Findings: WavLM vs Wav2Vec2 -->
            <section>
                <h2>Domain Probe Findings: Backbone Comparison</h2>
                <p>Linear probe accuracy predicting codec from frozen SSL features (3-class: MP3/AAC/OPUS):</p>
                <div class="two-col" style="margin-top: 0.3em;">
                    <div class="col">
                        <img src="codec_encoding_heatmap.jpg" alt="Codec Encoding Heatmap" style="width: 100%; border-radius: 6px; border: 1px solid var(--border);">
                        <p class="small" style="margin-top: 0.3em; text-align: center;">Chance = 33.3% (3-class). Wav2Vec2 late layers approach chance.</p>
                    </div>
                    <div class="col">
                        <h3>Key Insights</h3>
                        <ul style="font-size: 0.85em;">
                            <li><strong>WavLM encodes MORE codec info</strong> — but DANN still works better with it</li>
                            <li>WavLM has <strong>uniform distribution</strong> across layers (94% → 72%)</li>
                            <li>Wav2Vec2 <strong>drops sharply</strong> (77% → 51%, near chance)</li>
                            <li>Gap between backbones <em>increases</em> in late layers</li>
                        </ul>
                        <div class="card card-accent" style="margin-top: 0.5em; font-size: 0.85em;">
                            <strong>Hypothesis:</strong> WavLM's uniform codec encoding makes it easier for DANN to find layer combinations that preserve task signal while suppressing domain info.
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- 17. Interpretability Analysis -->
            <section>
                <h2>Interpretability Analysis</h2>
                <h3>Layer-wise Domain Probing</h3>
                <ul>
                    <li>Train linear probe at each layer to predict CODEC/CODEC_Q</li>
                    <li>Compare accuracy: ERM vs DANN</li>
                    <li>Lower probe accuracy in DANN = less domain leakage</li>
                </ul>
                <h3 style="margin-top: 0.6em;">Representation Similarity (CKA)</h3>
                <ul>
                    <li>Compare layer representations between ERM and DANN</li>
                    <li>Identify where DANN diverges most from baseline</li>
                </ul>
                <h3 style="margin-top: 0.6em;">Activation Patching (RQ4)</h3>
                <ul>
                    <li>Swap activations from DANN into ERM at identified layers</li>
                    <li>Test if this reduces domain leakage without full retraining</li>
                </ul>
            </section>
            
            <!-- 18. Status -->
            <section>
                <h2>Status & Next Steps</h2>
                <div class="two-col">
                    <div class="col">
                        <h3>Completed</h3>
                        <ul class="checklist">
                            <li class="done">Full ERM + DANN codebase</li>
                            <li class="done">Multi-head domain discriminator</li>
                            <li class="done">Pre-compute augmentation (1.6M files)</li>
                            <li class="done">Lambda stability fix (PR #62)</li>
                            <li class="done">WavLM vs Wav2Vec2 probe analysis</li>
                            <li class="done">Codec coverage analysis (79.2%)</li>
                            <li class="done">DANN training runs (both backbones)</li>
                            <li class="done">60+ unit tests, W&B, Snellius jobs</li>
                        </ul>
                    </div>
                    <div class="col">
                        <h3>In Progress</h3>
                        <ul class="checklist">
                            <li class="pending">Full eval set evaluation (680k samples)</li>
                            <li class="pending">Per-codec EER breakdown</li>
                        </ul>
                        <h3 style="margin-top: 0.6em;">Upcoming</h3>
                        <ul class="checklist">
                            <li class="future">ERM vs DANN comparison analysis</li>
                            <li class="future">CKA representation similarity</li>
                            <li class="future">Activation patching experiments</li>
                            <li class="future">Thesis writing</li>
                        </ul>
                    </div>
                </div>
            </section>
            
            <!-- 19. Questions -->
            <section class="center" style="display: flex; flex-direction: column; justify-content: center;">
                <h2>Questions?</h2>
                <p style="margin-top: 2em; color: var(--text-secondary); font-size: 0.9em;">
                    <strong>Repository:</strong> github.com/Jmqcooper1/asvspoof5-domain-invariant-cm<br>
                    <strong>W&B:</strong> mike-cooper-uva/asvspoof5-dann
                </p>
            </section>
            
        </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: 'c/t',
            transition: 'none',
            width: 1100,
            height: 680,
            margin: 0.08,
            center: false
        });
    </script>
</body>
</html>
