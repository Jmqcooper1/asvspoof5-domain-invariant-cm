{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thesis Results Generation\n",
        "\n",
        "This notebook generates all tables and figures for the thesis from wandb experiment tracking.\n",
        "\n",
        "**Tables:**\n",
        "- T1: Overall Results (all models, all metrics)\n",
        "- T2: OOD Gap (dev vs eval, gap reduction %)\n",
        "- T3: Per-codec breakdown\n",
        "- T4: Probe accuracy per layer\n",
        "\n",
        "**Figures:**\n",
        "- F1: Per-codec EER grouped bar chart\n",
        "- F2: Layer-wise probe accuracy line plot\n",
        "- F3: CKA heatmap placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install wandb pandas matplotlib seaborn pyyaml"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "\n",
        "# Thesis-appropriate styling\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({\n",
        "    'font.size': 11,\n",
        "    'font.family': 'serif',\n",
        "    'axes.labelsize': 12,\n",
        "    'axes.titlesize': 13,\n",
        "    'legend.fontsize': 10,\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'savefig.bbox': 'tight',\n",
        "    'figure.figsize': (10, 6),\n",
        "})\n",
        "\n",
        "# Color palette for models\n",
        "COLORS = {\n",
        "    'wavlm_erm': '#1f77b4',    # Blue\n",
        "    'wavlm_dann': '#ff7f0e',   # Orange\n",
        "    'w2v2_erm': '#2ca02c',     # Green\n",
        "    'w2v2_dann': '#d62728',    # Red\n",
        "}\n",
        "\n",
        "MODEL_LABELS = {\n",
        "    'wavlm_erm': 'WavLM ERM',\n",
        "    'wavlm_dann': 'WavLM DANN',\n",
        "    'w2v2_erm': 'Wav2Vec2 ERM',\n",
        "    'w2v2_dann': 'Wav2Vec2 DANN',\n",
        "}\n",
        "\n",
        "# Codec names for clarity\n",
        "CODEC_NAMES = {\n",
        "    'C01': 'AMR-WB',\n",
        "    'C02': 'EVS',\n",
        "    'C03': 'G.722',\n",
        "    'C04': 'G.726',\n",
        "    'C05': 'GSM-FR',\n",
        "    'C06': 'iLBC',\n",
        "    'C07': 'MP3',\n",
        "    'C08': 'Opus',\n",
        "    'C09': 'Speex',\n",
        "    'C10': 'Vorbis',\n",
        "    'C11': 'μ-law',\n",
        "    'NONE': 'Uncoded',\n",
        "}\n",
        "\n",
        "# print(f\"Setup complete. Using matplotlib {matplotlib.__version__}\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Output paths configuration\n",
        "REPO_ROOT = Path('../').resolve()\n",
        "OUTPUT_DIR = REPO_ROOT / 'outputs'\n",
        "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
        "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
        "\n",
        "# Create output directories\n",
        "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Output directories created:\")\n",
        "print(f\"  Tables: {TABLES_DIR}\")\n",
        "print(f\"  Figures: {FIGURES_DIR}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output directories created:\n",
            "  Tables: /Users/jmqcooper/Documents/Development/asvspoof5-domain-invariant-cm/outputs/tables\n",
            "  Figures: /Users/jmqcooper/Documents/Development/asvspoof5-domain-invariant-cm/outputs/figures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Wandb API connection\n",
        "WANDB_API_KEY_PATH = Path.home() / '.wandb_api_key'\n",
        "WANDB_ENTITY = 'mike-cooper-uva'\n",
        "WANDB_PROJECT = 'asvspoof5-dann'\n",
        "\n",
        "# Load API key\n",
        "if WANDB_API_KEY_PATH.exists():\n",
        "    with open(WANDB_API_KEY_PATH) as f:\n",
        "        wandb_api_key = f.read().strip()\n",
        "    os.environ['WANDB_API_KEY'] = wandb_api_key\n",
        "    print(f\"Loaded wandb API key from {WANDB_API_KEY_PATH}\")\n",
        "else:\n",
        "    print(\"Warning: wandb API key not found. Using environment variable or login.\")\n",
        "\n",
        "# Initialize API\n",
        "api = wandb.Api()\n",
        "print(f\"Connected to wandb project: {WANDB_ENTITY}/{WANDB_PROJECT}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: wandb API key not found. Using environment variable or login.\n",
            "Connected to wandb project: mike-cooper-uva/asvspoof5-dann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading from Wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run IDs for eval runs (full per-codec metrics)\n",
        "EVAL_RUN_IDS = {\n",
        "    'wavlm_erm': '7ncrwi99',\n",
        "    'wavlm_dann': 'aaogtffx',\n",
        "    'w2v2_erm': 'strsigcn',\n",
        "    'w2v2_dann': 'v4p7t176',\n",
        "}\n",
        "\n",
        "# Dev run IDs (for OOD gap calculation)\n",
        "DEV_RUN_IDS = {\n",
        "    'wavlm_erm': 'txeq8b8p',\n",
        "    'wavlm_dann': '5ltu7x9f',\n",
        "    'w2v2_erm': 'e0mx9gzt',\n",
        "    'w2v2_dann': 'pv9m2g8o',\n",
        "}\n",
        "\n",
        "# Baseline run IDs\n",
        "BASELINE_RUN_IDS = {\n",
        "    'lfcc_gmm': '9zrocjqe',\n",
        "    'trillsson_logistic': '5u85m3fu',\n",
        "    'trillsson_mlp': '28qetqki',\n",
        "}\n",
        "\n",
        "# Probe comparison run IDs\n",
        "PROBE_RUN_IDS = {\n",
        "    'probes_comparison_1': 'evyluap2',\n",
        "    'probes_comparison_2': 'heo6rsy9',\n",
        "}\n",
        "\n",
        "print(\"Run IDs configured\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run IDs configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_run(run_id: str) -> wandb.apis.public.Run:\n",
        "    \"\"\"Load a single run by ID.\"\"\"\n",
        "    return api.run(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{run_id}\")\n",
        "\n",
        "def get_run_summary(run: wandb.apis.public.Run) -> Dict[str, Any]:\n",
        "    \"\"\"Extract summary metrics from a run.\"\"\"\n",
        "    return dict(run.summary)\n",
        "\n",
        "def get_run_config(run: wandb.apis.public.Run) -> Dict[str, Any]:\n",
        "    \"\"\"Extract config from a run.\"\"\"\n",
        "    return dict(run.config)\n",
        "\n",
        "def format_optional_float(value: Optional[Any], precision: int = 4) -> str:\n",
        "    \"\"\"Format numeric-like values safely for display in tables/prints.\"\"\"\n",
        "    if value is None:\n",
        "        return \"N/A\"\n",
        "\n",
        "    try:\n",
        "        numeric_value = float(value)\n",
        "    except (TypeError, ValueError):\n",
        "        return \"N/A\"\n",
        "\n",
        "    return f\"{numeric_value:.{precision}f}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load eval runs\n",
        "print(\"Loading eval runs...\")\n",
        "eval_runs = {}\n",
        "eval_summaries = {}\n",
        "\n",
        "for name, run_id in EVAL_RUN_IDS.items():\n",
        "    try:\n",
        "        run = load_run(run_id)\n",
        "        eval_runs[name] = run\n",
        "        eval_summaries[name] = get_run_summary(run)\n",
        "        print(f\"  ✓ Loaded {name} ({run_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Failed to load {name} ({run_id}): {e}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(eval_runs)} eval runs\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading eval runs...\n",
            "  ✓ Loaded wavlm_erm (7ncrwi99)\n",
            "  ✓ Loaded wavlm_dann (aaogtffx)\n",
            "  ✓ Loaded w2v2_erm (strsigcn)\n",
            "  ✓ Loaded w2v2_dann (v4p7t176)\n",
            "\n",
            "Loaded 4 eval runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load dev runs (for OOD gap)\n",
        "print(\"Loading dev runs...\")\n",
        "dev_runs = {}\n",
        "dev_summaries = {}\n",
        "\n",
        "for name, run_id in DEV_RUN_IDS.items():\n",
        "    try:\n",
        "        run = load_run(run_id)\n",
        "        dev_runs[name] = run\n",
        "        dev_summaries[name] = get_run_summary(run)\n",
        "        print(f\"  ✓ Loaded {name} ({run_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Failed to load {name} ({run_id}): {e}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(dev_runs)} dev runs\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dev runs...\n",
            "  ✓ Loaded wavlm_erm (txeq8b8p)\n",
            "  ✓ Loaded wavlm_dann (5ltu7x9f)\n",
            "  ✓ Loaded w2v2_erm (e0mx9gzt)\n",
            "  ✓ Loaded w2v2_dann (pv9m2g8o)\n",
            "\n",
            "Loaded 4 dev runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load baseline runs\n",
        "print(\"Loading baseline runs...\")\n",
        "baseline_runs = {}\n",
        "baseline_summaries = {}\n",
        "\n",
        "for name, run_id in BASELINE_RUN_IDS.items():\n",
        "    try:\n",
        "        run = load_run(run_id)\n",
        "        baseline_runs[name] = run\n",
        "        baseline_summaries[name] = get_run_summary(run)\n",
        "        print(f\"  ✓ Loaded {name} ({run_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Failed to load {name} ({run_id}): {e}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(baseline_runs)} baseline runs\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading baseline runs...\n",
            "  ✓ Loaded lfcc_gmm (9zrocjqe)\n",
            "  ✓ Loaded trillsson_logistic (5u85m3fu)\n",
            "  ✓ Loaded trillsson_mlp (28qetqki)\n",
            "\n",
            "Loaded 3 baseline runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load probe runs\n",
        "print(\"Loading probe comparison runs...\")\n",
        "probe_runs = {}\n",
        "probe_summaries = {}\n",
        "\n",
        "for name, run_id in PROBE_RUN_IDS.items():\n",
        "    try:\n",
        "        run = load_run(run_id)\n",
        "        probe_runs[name] = run\n",
        "        probe_summaries[name] = get_run_summary(run)\n",
        "        print(f\"  ✓ Loaded {name} ({run_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Failed to load {name} ({run_id}): {e}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(probe_runs)} probe runs\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading probe comparison runs...\n",
            "  ✓ Loaded probes_comparison_1 (evyluap2)\n",
            "  ✓ Loaded probes_comparison_2 (heo6rsy9)\n",
            "\n",
            "Loaded 2 probe runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Extract Overall Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_overall_metrics(summary: Dict[str, Any], split: str = 'eval') -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extract overall metrics from a run summary.\n",
        "    \n",
        "    Args:\n",
        "        summary: Run summary dict\n",
        "        split: 'eval' or 'dev'\n",
        "    \n",
        "    Returns:\n",
        "        Dict with eer, min_dcf, auc, f1_macro\n",
        "    \"\"\"\n",
        "    prefix = f'eval/{split}/'\n",
        "    \n",
        "    return {\n",
        "        'eer': summary.get(f'{prefix}eer', summary.get('eval/eer')),\n",
        "        'min_dcf': summary.get(f'{prefix}min_dcf', summary.get('eval/min_dcf')),\n",
        "        'auc': summary.get(f'{prefix}auc', summary.get('eval/auc')),\n",
        "        'f1_macro': summary.get(f'{prefix}f1_macro', summary.get('eval/f1_macro')),\n",
        "    }\n",
        "\n",
        "# Extract metrics for all models\n",
        "eval_metrics = {name: extract_overall_metrics(summary, 'eval') \n",
        "                for name, summary in eval_summaries.items()}\n",
        "dev_metrics = {name: extract_overall_metrics(summary, 'dev') \n",
        "               for name, summary in dev_summaries.items()}\n",
        "\n",
        "print(\"Eval metrics:\")\n",
        "for name, metrics in eval_metrics.items():\n",
        "    eer_value = metrics[\"eer\"]\n",
        "    eer_display = format_optional_float(eer_value)\n",
        "    print(f\"  {name}: EER={eer_display}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval metrics:\n",
            "  wavlm_erm: EER=0.0848\n",
            "  wavlm_dann: EER=0.0736\n",
            "  w2v2_erm: EER=0.1515\n",
            "  w2v2_dann: EER=0.1437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Extract Per-Codec Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_per_codec_metrics(summary: Dict[str, Any]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Extract per-codec metrics from a run summary.\n",
        "    \n",
        "    Returns:\n",
        "        Dict[codec] -> {eer, auc, f1_macro}\n",
        "    \"\"\"\n",
        "    codecs = ['C01', 'C02', 'C03', 'C04', 'C05', 'C06', \n",
        "              'C07', 'C08', 'C09', 'C10', 'C11', 'NONE']\n",
        "    \n",
        "    result = {}\n",
        "    for codec in codecs:\n",
        "        result[codec] = {\n",
        "            'eer': summary.get(f'eval/eval/codec/{codec}/eer'),\n",
        "            'auc': summary.get(f'eval/eval/codec/{codec}/auc'),\n",
        "            'f1_macro': summary.get(f'eval/eval/codec/{codec}/f1_macro'),\n",
        "        }\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Extract per-codec metrics for all models\n",
        "per_codec_metrics = {name: extract_per_codec_metrics(summary) \n",
        "                     for name, summary in eval_summaries.items()}\n",
        "\n",
        "# Display sample\n",
        "print(\"Sample per-codec metrics (wavlm_erm):\")\n",
        "if \"wavlm_erm\" in per_codec_metrics:\n",
        "    for codec in [\"C01\", \"C07\", \"NONE\"]:\n",
        "        codec_metrics = per_codec_metrics[\"wavlm_erm\"].get(codec, {})\n",
        "        eer_value = codec_metrics.get(\"eer\")\n",
        "        eer_display = format_optional_float(eer_value)\n",
        "        print(f\"  {codec}: EER={eer_display}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample per-codec metrics (wavlm_erm):\n",
            "  C01: EER=0.0753\n",
            "  C07: EER=0.1165\n",
            "  NONE: EER=0.0612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Extract Probe Accuracy by Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_probe_accuracy(summary: Dict[str, Any]) -> Dict[str, Dict[int, float]]:\n",
        "    \"\"\"\n",
        "    Extract probe accuracy per layer from a run summary.\n",
        "    \n",
        "    Returns:\n",
        "        Dict[method] -> Dict[layer] -> accuracy\n",
        "    \"\"\"\n",
        "    result = {'erm': {}, 'dann': {}}\n",
        "    \n",
        "    for layer in range(12):\n",
        "        # Try different key patterns\n",
        "        erm_key = f'probe/erm/codec/layer_{layer}'\n",
        "        dann_key = f'probe/dann/codec/layer_{layer}'\n",
        "        \n",
        "        result['erm'][layer] = summary.get(erm_key)\n",
        "        result['dann'][layer] = summary.get(dann_key)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Extract probe accuracy from probe runs\n",
        "probe_accuracy = {}\n",
        "for name, summary in probe_summaries.items():\n",
        "    probe_accuracy[name] = extract_probe_accuracy(summary)\n",
        "\n",
        "print(\"Probe accuracy extracted\")\n",
        "# Show sample if available\n",
        "for name, acc in probe_accuracy.items():\n",
        "    erm_layer0_accuracy = acc.get(\"erm\", {}).get(0)\n",
        "    dann_layer0_accuracy = acc.get(\"dann\", {}).get(0)\n",
        "\n",
        "    if erm_layer0_accuracy is not None:\n",
        "        erm_display = format_optional_float(erm_layer0_accuracy)\n",
        "        dann_display = format_optional_float(dann_layer0_accuracy)\n",
        "        print(f\"\\n{name} - Layer 0:\")\n",
        "        print(f\"  ERM: {erm_display}\")\n",
        "        print(f\"  DANN: {dann_display}\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probe accuracy extracted\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown format code 'f' for object of type 'str'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m dann_layer0_accuracy = acc.get(\u001b[33m\"\u001b[39m\u001b[33mdann\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[32m0\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m erm_layer0_accuracy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     erm_display = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43merm_layer0_accuracy\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m     dann_display = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdann_layer0_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dann_layer0_accuracy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Layer 0:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mValueError\u001b[39m: Unknown format code 'f' for object of type 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Build Overall Results DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_overall_results_df() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build DataFrame with overall results for all models.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    \n",
        "    # Main models\n",
        "    for model_key, label in MODEL_LABELS.items():\n",
        "        backbone = 'WavLM' if 'wavlm' in model_key else 'Wav2Vec2'\n",
        "        method = 'DANN' if 'dann' in model_key else 'ERM'\n",
        "        \n",
        "        # Get eval metrics\n",
        "        eval_m = eval_metrics.get(model_key, {})\n",
        "        dev_m = dev_metrics.get(model_key, {})\n",
        "        \n",
        "        rows.append({\n",
        "            'Model': backbone,\n",
        "            'Method': method,\n",
        "            'Dev EER': dev_m.get('eer'),\n",
        "            'Eval EER': eval_m.get('eer'),\n",
        "            'minDCF': eval_m.get('min_dcf'),\n",
        "            'AUC': eval_m.get('auc'),\n",
        "            'F1': eval_m.get('f1_macro'),\n",
        "        })\n",
        "    \n",
        "    # Baselines\n",
        "    baseline_configs = [\n",
        "        ('lfcc_gmm', 'LFCC-GMM', 'Baseline'),\n",
        "        ('trillsson_logistic', 'Trillsson', 'Logistic'),\n",
        "        ('trillsson_mlp', 'Trillsson', 'MLP'),\n",
        "    ]\n",
        "    \n",
        "    for key, model, method in baseline_configs:\n",
        "        if key in baseline_summaries:\n",
        "            summary = baseline_summaries[key]\n",
        "            rows.append({\n",
        "                'Model': model,\n",
        "                'Method': method,\n",
        "                'Dev EER': summary.get('eval/dev/eer', summary.get('eval/eer')),\n",
        "                'Eval EER': summary.get('eval/eval/eer'),\n",
        "                'minDCF': summary.get('eval/eval/min_dcf', summary.get('eval/dev/min_dcf', summary.get('min_dcf'))),\n",
        "                'AUC': summary.get('eval/eval/auc', summary.get('eval/dev/auc', summary.get('auc'))),\n",
        "                'F1': summary.get('eval/eval/f1_macro', summary.get('eval/dev/f1_macro')),\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "df_overall = build_overall_results_df()\n",
        "print(\"Overall Results:\")\n",
        "df_overall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Compute OOD Gap Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_ood_gap_df() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build DataFrame showing OOD gap (dev vs eval) and gap reduction from DANN.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    \n",
        "    for backbone in ['wavlm', 'w2v2']:\n",
        "        backbone_label = 'WavLM' if backbone == 'wavlm' else 'Wav2Vec2'\n",
        "        \n",
        "        erm_key = f'{backbone}_erm'\n",
        "        dann_key = f'{backbone}_dann'\n",
        "        \n",
        "        # Get ERM metrics\n",
        "        erm_dev_eer = dev_metrics.get(erm_key, {}).get('eer')\n",
        "        erm_eval_eer = eval_metrics.get(erm_key, {}).get('eer')\n",
        "        \n",
        "        # Get DANN metrics\n",
        "        dann_dev_eer = dev_metrics.get(dann_key, {}).get('eer')\n",
        "        dann_eval_eer = eval_metrics.get(dann_key, {}).get('eer')\n",
        "        \n",
        "        # Calculate gaps\n",
        "        erm_gap = (erm_eval_eer - erm_dev_eer) if (erm_eval_eer and erm_dev_eer) else None\n",
        "        dann_gap = (dann_eval_eer - dann_dev_eer) if (dann_eval_eer and dann_dev_eer) else None\n",
        "        \n",
        "        # Gap reduction\n",
        "        gap_reduction = ((erm_gap - dann_gap) / erm_gap * 100) if (erm_gap and dann_gap) else None\n",
        "        \n",
        "        # ERM row\n",
        "        rows.append({\n",
        "            'Model': backbone_label,\n",
        "            'Method': 'ERM',\n",
        "            'Dev EER': erm_dev_eer,\n",
        "            'Eval EER': erm_eval_eer,\n",
        "            'Gap (pp)': erm_gap,\n",
        "            'Gap Reduction': '-',\n",
        "        })\n",
        "        \n",
        "        # DANN row\n",
        "        rows.append({\n",
        "            'Model': backbone_label,\n",
        "            'Method': 'DANN',\n",
        "            'Dev EER': dann_dev_eer,\n",
        "            'Eval EER': dann_eval_eer,\n",
        "            'Gap (pp)': dann_gap,\n",
        "            'Gap Reduction': f\"{format_optional_float(gap_reduction, precision=1)}%\" if gap_reduction is not None else \"N/A\",\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_ood_gap = build_ood_gap_df()\n",
        "print(\"OOD Gap Analysis:\")\n",
        "df_ood_gap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Build Per-Codec DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_per_codec_df() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build DataFrame with per-codec EER for all models.\n",
        "    \"\"\"\n",
        "    codecs = ['C01', 'C02', 'C03', 'C04', 'C05', 'C06', \n",
        "              'C07', 'C08', 'C09', 'C10', 'C11', 'NONE']\n",
        "    \n",
        "    rows = []\n",
        "    for codec in codecs:\n",
        "        row = {'Codec': codec, 'Name': CODEC_NAMES.get(codec, codec)}\n",
        "        \n",
        "        for model_key in ['wavlm_erm', 'wavlm_dann', 'w2v2_erm', 'w2v2_dann']:\n",
        "            if model_key in per_codec_metrics:\n",
        "                eer = per_codec_metrics[model_key].get(codec, {}).get('eer')\n",
        "                row[MODEL_LABELS[model_key]] = eer\n",
        "        \n",
        "        rows.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    \n",
        "    # Calculate deltas if both ERM and DANN exist\n",
        "    if 'WavLM ERM' in df.columns and 'WavLM DANN' in df.columns:\n",
        "        df['WavLM Δ'] = df['WavLM DANN'] - df['WavLM ERM']\n",
        "    \n",
        "    if 'Wav2Vec2 ERM' in df.columns and 'Wav2Vec2 DANN' in df.columns:\n",
        "        df['W2V2 Δ'] = df['Wav2Vec2 DANN'] - df['Wav2Vec2 ERM']\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_per_codec = build_per_codec_df()\n",
        "print(\"Per-Codec EER:\")\n",
        "df_per_codec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Build Probe Accuracy DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_probe_accuracy_df() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build DataFrame with probe accuracy per layer.\n",
        "    Uses first available probe run with data.\n",
        "    \"\"\"\n",
        "    # Find probe data\n",
        "    probe_data = None\n",
        "    for name, acc in probe_accuracy.items():\n",
        "        if acc['erm'].get(0) is not None:\n",
        "            probe_data = acc\n",
        "            break\n",
        "    \n",
        "    if probe_data is None:\n",
        "        # Use hardcoded values from plan if wandb data unavailable\n",
        "        probe_data = {\n",
        "            'erm': {0: 0.7810, 1: 0.7642, 2: 0.7462, 3: 0.6712, 4: 0.5790,\n",
        "                    5: 0.4652, 6: 0.3790, 7: 0.3188, 8: 0.3034, 9: 0.3066,\n",
        "                    10: 0.3262, 11: 0.3304},\n",
        "            'dann': {0: 0.7554, 1: 0.7430, 2: 0.6994, 3: 0.6008, 4: 0.5074,\n",
        "                     5: 0.4322, 6: 0.3666, 7: 0.3264, 8: 0.3096, 9: 0.3096,\n",
        "                     10: 0.3288, 11: 0.3304},\n",
        "        }\n",
        "        print(\"Using hardcoded probe data from plan (wandb data not available)\")\n",
        "    \n",
        "    rows = []\n",
        "    for layer in range(12):\n",
        "        erm_acc = probe_data['erm'].get(layer)\n",
        "        dann_acc = probe_data['dann'].get(layer)\n",
        "        \n",
        "        reduction = None\n",
        "        if erm_acc and dann_acc:\n",
        "            reduction = (erm_acc - dann_acc) / erm_acc * 100\n",
        "        \n",
        "        rows.append({\n",
        "            'Layer': layer,\n",
        "            'ERM Codec Acc': erm_acc,\n",
        "            'DANN Codec Acc': dann_acc,\n",
        "            'Reduction (%)': reduction,\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_probe = build_probe_accuracy_df()\n",
        "print(\"Probe Accuracy per Layer:\")\n",
        "df_probe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Table Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def format_percentage(val, decimals=2):\n",
        "    \"\"\"Format value as percentage.\"\"\"\n",
        "    if pd.isna(val) or val is None:\n",
        "        return '-'\n",
        "    return f\"{val * 100:.{decimals}f}%\"\n",
        "\n",
        "def format_float(val, decimals=3):\n",
        "    \"\"\"Format float value.\"\"\"\n",
        "    if pd.isna(val) or val is None:\n",
        "        return '-'\n",
        "    return f\"{val:.{decimals}f}\"\n",
        "\n",
        "def to_latex_booktabs(df: pd.DataFrame, caption: str, label: str, \n",
        "                       column_format: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Convert DataFrame to LaTeX booktabs format.\n",
        "    \"\"\"\n",
        "    if column_format is None:\n",
        "        column_format = 'l' + 'c' * (len(df.columns) - 1)\n",
        "    \n",
        "    latex = df.to_latex(\n",
        "        index=False,\n",
        "        escape=True,\n",
        "        column_format=column_format,\n",
        "    )\n",
        "    \n",
        "    # Wrap in table environment with caption\n",
        "    wrapped = f\"\"\"\\\\begin{{table}}[htbp]\n",
        "\\\\centering\n",
        "\\\\caption{{{caption}}}\n",
        "\\\\label{{{label}}}\n",
        "{latex}\\\\end{{table}}\"\"\"\n",
        "    \n",
        "    return wrapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T1: Overall Results Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format the overall results table\n",
        "df_t1 = df_overall.copy()\n",
        "df_t1['Dev EER'] = df_t1['Dev EER'].apply(lambda x: format_percentage(x))\n",
        "df_t1['Eval EER'] = df_t1['Eval EER'].apply(lambda x: format_percentage(x))\n",
        "df_t1['minDCF'] = df_t1['minDCF'].apply(lambda x: format_float(x))\n",
        "df_t1['AUC'] = df_t1['AUC'].apply(lambda x: format_float(x))\n",
        "df_t1['F1'] = df_t1['F1'].apply(lambda x: format_float(x))\n",
        "\n",
        "print(\"Table T1: Overall Results\")\n",
        "display(df_t1)\n",
        "\n",
        "# Generate LaTeX\n",
        "latex_t1 = to_latex_booktabs(\n",
        "    df_t1,\n",
        "    caption='Overall performance comparison of domain adaptation methods on ASVspoof5.',\n",
        "    label='tab:overall-results'\n",
        ")\n",
        "\n",
        "# Save\n",
        "with open(TABLES_DIR / 't1_overall_results.tex', 'w') as f:\n",
        "    f.write(latex_t1)\n",
        "df_t1.to_csv(TABLES_DIR / 't1_overall_results.csv', index=False)\n",
        "\n",
        "print(f\"\\nSaved to {TABLES_DIR / 't1_overall_results.tex'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T2: OOD Gap Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format the OOD gap table\n",
        "df_t2 = df_ood_gap.copy()\n",
        "df_t2['Dev EER'] = df_t2['Dev EER'].apply(lambda x: format_percentage(x))\n",
        "df_t2['Eval EER'] = df_t2['Eval EER'].apply(lambda x: format_percentage(x))\n",
        "df_t2['Gap (pp)'] = df_t2['Gap (pp)'].apply(lambda x: f\"+{x*100:.2f}\" if x and x > 0 else (f\"{x*100:.2f}\" if x else '-'))\n",
        "\n",
        "print(\"Table T2: OOD Gap Analysis\")\n",
        "display(df_t2)\n",
        "\n",
        "# Generate LaTeX\n",
        "latex_t2 = to_latex_booktabs(\n",
        "    df_t2,\n",
        "    caption='Out-of-distribution generalization gap between development and evaluation sets.',\n",
        "    label='tab:ood-gap'\n",
        ")\n",
        "\n",
        "# Save\n",
        "with open(TABLES_DIR / 't2_ood_gap.tex', 'w') as f:\n",
        "    f.write(latex_t2)\n",
        "df_t2.to_csv(TABLES_DIR / 't2_ood_gap.csv', index=False)\n",
        "\n",
        "print(f\"\\nSaved to {TABLES_DIR / 't2_ood_gap.tex'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T3: Per-Codec Breakdown Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format the per-codec table\n",
        "df_t3 = df_per_codec.copy()\n",
        "\n",
        "# Format EER columns as percentages\n",
        "eer_cols = [col for col in df_t3.columns if col not in ['Codec', 'Name', 'WavLM Δ', 'W2V2 Δ']]\n",
        "for col in eer_cols:\n",
        "    df_t3[col] = df_t3[col].apply(lambda x: format_percentage(x))\n",
        "\n",
        "# Format delta columns\n",
        "if 'WavLM Δ' in df_t3.columns:\n",
        "    df_t3['WavLM Δ'] = df_t3['WavLM Δ'].apply(lambda x: f\"{x*100:+.2f}\" if pd.notna(x) else '-')\n",
        "if 'W2V2 Δ' in df_t3.columns:\n",
        "    df_t3['W2V2 Δ'] = df_t3['W2V2 Δ'].apply(lambda x: f\"{x*100:+.2f}\" if pd.notna(x) else '-')\n",
        "\n",
        "print(\"Table T3: Per-Codec EER Breakdown\")\n",
        "display(df_t3)\n",
        "\n",
        "# Generate LaTeX\n",
        "latex_t3 = to_latex_booktabs(\n",
        "    df_t3,\n",
        "    caption='Per-codec Equal Error Rate (EER) on the ASVspoof5 evaluation set.',\n",
        "    label='tab:per-codec'\n",
        ")\n",
        "\n",
        "# Save\n",
        "with open(TABLES_DIR / 't3_per_codec.tex', 'w') as f:\n",
        "    f.write(latex_t3)\n",
        "df_t3.to_csv(TABLES_DIR / 't3_per_codec.csv', index=False)\n",
        "\n",
        "print(f\"\\nSaved to {TABLES_DIR / 't3_per_codec.tex'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T4: Probe Accuracy per Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format the probe accuracy table\n",
        "df_t4 = df_probe.copy()\n",
        "df_t4['ERM Codec Acc'] = df_t4['ERM Codec Acc'].apply(lambda x: format_percentage(x))\n",
        "df_t4['DANN Codec Acc'] = df_t4['DANN Codec Acc'].apply(lambda x: format_percentage(x))\n",
        "df_t4['Reduction (%)'] = df_t4['Reduction (%)'].apply(lambda x: f\"{x:.1f}%\" if pd.notna(x) else '-')\n",
        "\n",
        "print(\"Table T4: Probe Accuracy per Layer\")\n",
        "display(df_t4)\n",
        "\n",
        "# Generate LaTeX\n",
        "latex_t4 = to_latex_booktabs(\n",
        "    df_t4,\n",
        "    caption='Codec probe accuracy per WavLM layer for ERM and DANN models.',\n",
        "    label='tab:probe-accuracy'\n",
        ")\n",
        "\n",
        "# Save\n",
        "with open(TABLES_DIR / 't4_probe_accuracy.tex', 'w') as f:\n",
        "    f.write(latex_t4)\n",
        "df_t4.to_csv(TABLES_DIR / 't4_probe_accuracy.csv', index=False)\n",
        "\n",
        "print(f\"\\nSaved to {TABLES_DIR / 't4_probe_accuracy.tex'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Figure Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F1: Per-Codec EER Grouped Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_per_codec_eer(df: pd.DataFrame, save_path: Optional[Path] = None):\n",
        "    \"\"\"\n",
        "    Create grouped bar chart of per-codec EER.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    \n",
        "    codecs = df['Codec'].tolist()\n",
        "    x = np.arange(len(codecs))\n",
        "    width = 0.2\n",
        "    \n",
        "    # Get model columns\n",
        "    model_cols = ['WavLM ERM', 'WavLM DANN', 'Wav2Vec2 ERM', 'Wav2Vec2 DANN']\n",
        "    model_keys = ['wavlm_erm', 'wavlm_dann', 'w2v2_erm', 'w2v2_dann']\n",
        "    \n",
        "    available_models = [(col, key) for col, key in zip(model_cols, model_keys) \n",
        "                        if col in df.columns and df[col].notna().any()]\n",
        "    \n",
        "    offsets = np.linspace(-width * (len(available_models)-1)/2, \n",
        "                          width * (len(available_models)-1)/2, \n",
        "                          len(available_models))\n",
        "    \n",
        "    for i, (col, key) in enumerate(available_models):\n",
        "        # Convert from raw values (may need *100 if stored as decimals)\n",
        "        values = df[col].values\n",
        "        # Check if values are already in percentage form\n",
        "        if isinstance(values[0], str):\n",
        "            # Parse percentage strings\n",
        "            values = [float(v.strip('%')) if v != '-' else np.nan for v in values]\n",
        "        else:\n",
        "            values = values * 100  # Convert to percentage\n",
        "        \n",
        "        ax.bar(x + offsets[i], values, width, \n",
        "               label=col, color=COLORS[key], edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    ax.set_xlabel('Codec')\n",
        "    ax.set_ylabel('Equal Error Rate (%)')\n",
        "    ax.set_title('Per-Codec Performance Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f\"{c}\\n{CODEC_NAMES.get(c, '')}\" for c in codecs], \n",
        "                       rotation=45, ha='right')\n",
        "    ax.legend(loc='upper left', framealpha=0.9)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        fig.savefig(save_path.with_suffix('.pdf'), format='pdf', dpi=300)\n",
        "        fig.savefig(save_path.with_suffix('.png'), format='png', dpi=300)\n",
        "        print(f\"Saved to {save_path}\")\n",
        "    \n",
        "    return fig, ax\n",
        "\n",
        "# Use the raw (unformatted) data\n",
        "fig1, ax1 = plot_per_codec_eer(df_per_codec, FIGURES_DIR / 'f1_per_codec_eer')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F2: Layer-wise Probe Accuracy Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_probe_accuracy(df: pd.DataFrame, save_path: Optional[Path] = None):\n",
        "    \"\"\"\n",
        "    Create line plot of probe accuracy across layers.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    layers = df['Layer'].values\n",
        "    \n",
        "    # Parse values if they're strings\n",
        "    erm_acc = df['ERM Codec Acc'].values\n",
        "    dann_acc = df['DANN Codec Acc'].values\n",
        "    \n",
        "    if isinstance(erm_acc[0], str):\n",
        "        erm_acc = np.array([float(v.strip('%')) if v != '-' else np.nan for v in erm_acc])\n",
        "        dann_acc = np.array([float(v.strip('%')) if v != '-' else np.nan for v in dann_acc])\n",
        "    else:\n",
        "        erm_acc = erm_acc * 100\n",
        "        dann_acc = dann_acc * 100\n",
        "    \n",
        "    # Plot lines\n",
        "    ax.plot(layers, erm_acc, 'o-', label='ERM', color=COLORS['wavlm_erm'], \n",
        "            linewidth=2, markersize=8)\n",
        "    ax.plot(layers, dann_acc, 's--', label='DANN', color=COLORS['wavlm_dann'], \n",
        "            linewidth=2, markersize=8)\n",
        "    \n",
        "    # Fill between where DANN reduces leakage\n",
        "    ax.fill_between(layers, erm_acc, dann_acc, \n",
        "                    where=erm_acc > dann_acc, \n",
        "                    alpha=0.3, color='green', label='DANN Reduction')\n",
        "    \n",
        "    # Chance level for 12-class codec classification (C01-C11 + NONE)\n",
        "    ax.axhline(y=100/12, color='gray', linestyle=':', linewidth=1.5, label='Chance (12-class)')\n",
        "    \n",
        "    ax.set_xlabel('Layer')\n",
        "    ax.set_ylabel('Codec Probe Accuracy (%)')\n",
        "    ax.set_title('Domain Information Leakage Across Layers')\n",
        "    ax.set_xticks(layers)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.set_ylim(0, 100)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        fig.savefig(save_path.with_suffix('.pdf'), format='pdf', dpi=300)\n",
        "        fig.savefig(save_path.with_suffix('.png'), format='png', dpi=300)\n",
        "        print(f\"Saved to {save_path}\")\n",
        "    \n",
        "    return fig, ax\n",
        "\n",
        "# Use raw data\n",
        "fig2, ax2 = plot_probe_accuracy(df_probe, FIGURES_DIR / 'f2_probe_accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F3: CKA Heatmap (Placeholder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_cka_heatmap(cka_matrix: Optional[np.ndarray] = None, \n",
        "                     save_path: Optional[Path] = None):\n",
        "    \"\"\"\n",
        "    Create CKA heatmap between ERM and DANN representations.\n",
        "    \n",
        "    If no matrix provided, creates a placeholder figure.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    if cka_matrix is None:\n",
        "        # Placeholder - synthetic data for illustration\n",
        "        # Real CKA matrix should be loaded from wandb or computed\n",
        "        np.random.seed(42)\n",
        "        # Create realistic-looking CKA matrix (high similarity on diagonal, decreasing off-diagonal)\n",
        "        n_layers = 12\n",
        "        cka_matrix = np.zeros((n_layers, n_layers))\n",
        "        for i in range(n_layers):\n",
        "            for j in range(n_layers):\n",
        "                distance = abs(i - j)\n",
        "                cka_matrix[i, j] = np.exp(-0.3 * distance) + np.random.normal(0, 0.05)\n",
        "        cka_matrix = np.clip(cka_matrix, 0, 1)\n",
        "        \n",
        "        ax.set_title('CKA Similarity: ERM vs DANN (Placeholder)', fontsize=14)\n",
        "        print(\"Note: Using placeholder CKA matrix. Replace with actual data from wandb or recompute.\")\n",
        "    else:\n",
        "        ax.set_title('CKA Similarity: ERM vs DANN', fontsize=14)\n",
        "    \n",
        "    # Create heatmap\n",
        "    im = ax.imshow(cka_matrix, cmap='RdYlBu_r', vmin=0, vmax=1, aspect='equal')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "    cbar.set_label('CKA Similarity', fontsize=12)\n",
        "    \n",
        "    # Labels\n",
        "    layer_labels = [f'L{i}' for i in range(cka_matrix.shape[0])]\n",
        "    ax.set_xticks(range(len(layer_labels)))\n",
        "    ax.set_yticks(range(len(layer_labels)))\n",
        "    ax.set_xticklabels(layer_labels)\n",
        "    ax.set_yticklabels(layer_labels)\n",
        "    ax.set_xlabel('ERM Layer')\n",
        "    ax.set_ylabel('DANN Layer')\n",
        "    \n",
        "    # Add values in cells\n",
        "    for i in range(cka_matrix.shape[0]):\n",
        "        for j in range(cka_matrix.shape[1]):\n",
        "            text_color = 'white' if cka_matrix[i, j] > 0.5 else 'black'\n",
        "            ax.text(j, i, f'{cka_matrix[i, j]:.2f}', ha='center', va='center', \n",
        "                   color=text_color, fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        fig.savefig(save_path.with_suffix('.pdf'), format='pdf', dpi=300)\n",
        "        fig.savefig(save_path.with_suffix('.png'), format='png', dpi=300)\n",
        "        print(f\"Saved to {save_path}\")\n",
        "    \n",
        "    return fig, ax\n",
        "\n",
        "# Generate placeholder CKA heatmap\n",
        "fig3, ax3 = plot_cka_heatmap(save_path=FIGURES_DIR / 'f3_cka_heatmap')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Export Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List all generated outputs\n",
        "print(\"=\"*60)\n",
        "print(\"GENERATED OUTPUTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nTables:\")\n",
        "for f in sorted(TABLES_DIR.glob('*')):\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(\"\\nFigures:\")\n",
        "for f in sorted(FIGURES_DIR.glob('*')):\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Findings Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_key_findings():\n",
        "    \"\"\"\n",
        "    Generate key findings for thesis text.\n",
        "    \"\"\"\n",
        "    findings = []\n",
        "    \n",
        "    # RQ1: Does DANN reduce OOD gap?\n",
        "    if 'wavlm_erm' in eval_metrics and 'wavlm_dann' in eval_metrics:\n",
        "        wavlm_erm_eval = eval_metrics['wavlm_erm'].get('eer')\n",
        "        wavlm_dann_eval = eval_metrics['wavlm_dann'].get('eer')\n",
        "        \n",
        "        if wavlm_erm_eval and wavlm_dann_eval:\n",
        "            improvement = (wavlm_erm_eval - wavlm_dann_eval) / wavlm_erm_eval * 100\n",
        "            findings.append(\n",
        "                f\"RQ1: DANN reduces WavLM eval EER by {improvement:.1f}% \"\n",
        "                f\"({wavlm_erm_eval*100:.2f}% → {wavlm_dann_eval*100:.2f}%)\"\n",
        "            )\n",
        "    \n",
        "    # RQ2: Per-codec analysis\n",
        "    if 'wavlm_erm' in per_codec_metrics and 'wavlm_dann' in per_codec_metrics:\n",
        "        improvements = []\n",
        "        for codec in ['C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'NONE']:\n",
        "            erm = per_codec_metrics['wavlm_erm'].get(codec, {}).get('eer')\n",
        "            dann = per_codec_metrics['wavlm_dann'].get(codec, {}).get('eer')\n",
        "            if erm and dann:\n",
        "                improvements.append((codec, (erm - dann) * 100))\n",
        "        \n",
        "        if improvements:\n",
        "            best = max(improvements, key=lambda x: x[1])\n",
        "            worst = min(improvements, key=lambda x: x[1])\n",
        "            avg = np.mean([x[1] for x in improvements])\n",
        "            findings.append(\n",
        "                f\"RQ2: Average per-codec improvement: {avg:.2f}pp. \"\n",
        "                f\"Best: {best[0]} ({best[1]:.2f}pp), Worst: {worst[0]} ({worst[1]:.2f}pp)\"\n",
        "            )\n",
        "    \n",
        "    # RQ3: Probe analysis\n",
        "    if not df_probe.empty:\n",
        "        erm_acc = df_probe['ERM Codec Acc'].values\n",
        "        dann_acc = df_probe['DANN Codec Acc'].values\n",
        "        \n",
        "        if not isinstance(erm_acc[0], str):\n",
        "            avg_reduction = np.mean(erm_acc - dann_acc) * 100\n",
        "            max_reduction_idx = np.argmax(erm_acc - dann_acc)\n",
        "            findings.append(\n",
        "                f\"RQ3: Average probe accuracy reduction: {avg_reduction:.2f}pp. \"\n",
        "                f\"Max reduction at layer {max_reduction_idx}\"\n",
        "            )\n",
        "    \n",
        "    return findings\n",
        "\n",
        "print(\"KEY FINDINGS FOR THESIS:\")\n",
        "print(\"-\" * 60)\n",
        "for finding in generate_key_findings():\n",
        "    print(f\"• {finding}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Notebook Complete\n",
        "\n",
        "All tables and figures have been generated and saved to:\n",
        "- `outputs/tables/` - LaTeX and CSV files\n",
        "- `outputs/figures/` - PDF (vector) and PNG (raster) files\n",
        "\n",
        "### Next Steps:\n",
        "1. Review generated tables and figures\n",
        "2. Update CKA heatmap with actual data (run `scripts/run_cka.py` or load from wandb artifact)\n",
        "3. Copy LaTeX tables into thesis document\n",
        "4. Include figures with `\\includegraphics`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}