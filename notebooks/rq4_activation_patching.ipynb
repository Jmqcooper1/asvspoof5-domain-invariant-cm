{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Activation Patching for Domain Invariance\n",
    "\n",
    "**Research Question:** Can activation patching reduce domain leakage without full retraining?\n",
    "\n",
    "**Hypothesis:** By identifying layers where DANN diverges most from ERM (via CKA), we can \"transplant\" domain-invariant representations into ERM at inference time.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **CKA Analysis** — Identify which layers show largest representation difference between ERM and DANN\n",
    "2. **Activation Patching** — During inference, replace ERM activations at layer L with DANN activations\n",
    "3. **Evaluation** — Measure domain probe accuracy and EER on patched models\n",
    "\n",
    "## Expected Outcome\n",
    "\n",
    "If successful, this provides a lightweight method to improve domain robustness without expensive DANN training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add project root to path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from asvspoof5_domain_invariant_cm.utils import load_checkpoint, get_device\n",
    "from asvspoof5_domain_invariant_cm.data import ASVspoof5Dataset, get_dataloader\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Checkpoints\n",
    "\n",
    "Load the ERM and DANN models (WavLM backbone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths to your checkpoint locations\n",
    "ERM_CHECKPOINT = Path(\"../runs/wavlm_erm/checkpoints/best.pt\")\n",
    "DANN_CHECKPOINT = Path(\"../runs/wavlm_dann_exp/checkpoints/best.pt\")  # Exponential schedule\n",
    "\n",
    "# Load models\n",
    "erm_model, erm_config = load_checkpoint(ERM_CHECKPOINT, device=device)\n",
    "dann_model, dann_config = load_checkpoint(DANN_CHECKPOINT, device=device)\n",
    "\n",
    "erm_model.eval()\n",
    "dann_model.eval()\n",
    "\n",
    "print(f\"ERM config: {erm_config.get('training', {}).get('method')}\")\n",
    "print(f\"DANN config: {dann_config.get('training', {}).get('method')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CKA Analysis\n",
    "\n",
    "Centered Kernel Alignment (CKA) measures representational similarity between layers.\n",
    "We compute CKA between ERM and DANN at each layer to identify where they diverge most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_CKA(X, Y):\n",
    "    \"\"\"Compute linear CKA between two representation matrices.\n",
    "    \n",
    "    Args:\n",
    "        X: (n_samples, n_features_x) - Representations from model 1\n",
    "        Y: (n_samples, n_features_y) - Representations from model 2\n",
    "    \n",
    "    Returns:\n",
    "        CKA similarity score in [0, 1]\n",
    "    \"\"\"\n",
    "    # Center the representations\n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    Y = Y - Y.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Compute Gram matrices\n",
    "    XXT = X @ X.T\n",
    "    YYT = Y @ Y.T\n",
    "    \n",
    "    # CKA = HSIC(X,Y) / sqrt(HSIC(X,X) * HSIC(Y,Y))\n",
    "    hsic_xy = (XXT * YYT).sum()\n",
    "    hsic_xx = (XXT * XXT).sum()\n",
    "    hsic_yy = (YYT * YYT).sum()\n",
    "    \n",
    "    return (hsic_xy / torch.sqrt(hsic_xx * hsic_yy)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_representations(model, dataloader, num_batches=10):\n",
    "    \"\"\"Extract representations from each backbone layer.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {layer_idx: tensor of shape (n_samples, hidden_dim)}\n",
    "    \"\"\"\n",
    "    layer_reps = {i: [] for i in range(12)}  # 12 transformer layers\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, total=num_batches)):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            \n",
    "            audio = batch['audio'].to(device)\n",
    "            \n",
    "            # Get hidden states from all layers\n",
    "            # TODO: Implement based on your model's forward method\n",
    "            # hidden_states = model.backbone(audio, output_hidden_states=True)\n",
    "            \n",
    "            # For now, placeholder\n",
    "            pass\n",
    "    \n",
    "    return {k: torch.cat(v, dim=0) for k, v in layer_reps.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load a subset of eval data for CKA analysis\n",
    "# eval_loader = get_dataloader(...)\n",
    "\n",
    "# Extract representations\n",
    "# erm_reps = extract_layer_representations(erm_model, eval_loader)\n",
    "# dann_reps = extract_layer_representations(dann_model, eval_loader)\n",
    "\n",
    "# Compute CKA per layer\n",
    "# cka_scores = []\n",
    "# for layer_idx in range(12):\n",
    "#     cka = linear_CKA(erm_reps[layer_idx], dann_reps[layer_idx])\n",
    "#     cka_scores.append(cka)\n",
    "#     print(f\"Layer {layer_idx}: CKA = {cka:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot CKA scores\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(range(12), cka_scores)\n",
    "# plt.xlabel('Layer')\n",
    "# plt.ylabel('CKA Similarity')\n",
    "# plt.title('ERM vs DANN Representation Similarity (CKA)')\n",
    "# plt.axhline(y=0.9, color='r', linestyle='--', label='High similarity threshold')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "#\n",
    "# # Identify most divergent layers (lowest CKA)\n",
    "# divergent_layers = np.argsort(cka_scores)[:3]\n",
    "# print(f\"Most divergent layers: {divergent_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Patching\n",
    "\n",
    "Replace ERM activations at specific layers with DANN activations during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchedModel(torch.nn.Module):\n",
    "    \"\"\"Model that patches activations from a donor model at specified layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, donor_model, patch_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model: The model to run inference on (ERM)\n",
    "            donor_model: The model to take activations from (DANN)\n",
    "            patch_layers: List of layer indices to patch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.donor_model = donor_model\n",
    "        self.patch_layers = set(patch_layers)\n",
    "        \n",
    "        # Register hooks for patching\n",
    "        self._donor_activations = {}\n",
    "        self._setup_hooks()\n",
    "    \n",
    "    def _setup_hooks(self):\n",
    "        \"\"\"Setup forward hooks to capture and replace activations.\"\"\"\n",
    "        # TODO: Implement hooks based on model architecture\n",
    "        # This requires knowing the specific layer names in your backbone\n",
    "        pass\n",
    "    \n",
    "    def forward(self, audio):\n",
    "        \"\"\"Forward pass with activation patching.\"\"\"\n",
    "        # 1. Run donor model to capture activations\n",
    "        with torch.no_grad():\n",
    "            _ = self.donor_model(audio)\n",
    "        \n",
    "        # 2. Run base model with patched activations (via hooks)\n",
    "        output = self.base_model(audio)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create patched model\n",
    "# Target the most divergent layers identified by CKA\n",
    "# patched_model = PatchedModel(\n",
    "#     base_model=erm_model,\n",
    "#     donor_model=dann_model,\n",
    "#     patch_layers=divergent_layers\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Compare:\n",
    "1. Original ERM\n",
    "2. Original DANN\n",
    "3. Patched ERM (with DANN activations at selected layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Compute EER and collect predictions for analysis.\"\"\"\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label']\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(audio)\n",
    "            scores = torch.softmax(outputs['logits'], dim=-1)[:, 1]  # P(bonafide)\n",
    "            \n",
    "            all_scores.extend(scores.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Compute EER\n",
    "    # TODO: Import your EER computation function\n",
    "    # eer = compute_eer(all_labels, all_scores)\n",
    "    # return eer, all_scores, all_labels\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation\n",
    "# print(\"Evaluating ERM...\")\n",
    "# erm_eer = evaluate_model(erm_model, eval_loader)\n",
    "# print(f\"ERM EER: {erm_eer:.2%}\")\n",
    "\n",
    "# print(\"Evaluating DANN...\")\n",
    "# dann_eer = evaluate_model(dann_model, eval_loader)\n",
    "# print(f\"DANN EER: {dann_eer:.2%}\")\n",
    "\n",
    "# print(\"Evaluating Patched ERM...\")\n",
    "# patched_eer = evaluate_model(patched_model, eval_loader)\n",
    "# print(f\"Patched ERM EER: {patched_eer:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Domain Probe on Patched Model\n",
    "\n",
    "Verify that patching reduces domain leakage by running codec probes on the patched representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run domain probes on patched model representations\n",
    "# Compare probe accuracy: ERM vs Patched vs DANN\n",
    "# Lower probe accuracy = more domain invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary table and visualizations\n",
    "# | Model       | Eval EER | Domain Probe Acc | Notes |\n",
    "# |-------------|----------|------------------|-------|\n",
    "# | ERM         | X.XX%    | XX.X%            | Baseline |\n",
    "# | DANN        | X.XX%    | XX.X%            | Full adversarial training |\n",
    "# | Patched ERM | X.XX%    | XX.X%            | Layers [X,Y,Z] patched |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- **Key finding:** [Does patching work? Which layers matter?]\n",
    "- **Trade-offs:** [Computational cost vs performance gain]\n",
    "- **Limitations:** [What doesn't work?]\n",
    "- **Future work:** [Potential extensions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
