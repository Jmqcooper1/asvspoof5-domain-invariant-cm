{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RQ4: Activation Patching for Domain Invariance\n",
        "\n",
        "**Research Question:** Can activation patching reduce domain leakage without full retraining?\n",
        "\n",
        "**Hypothesis:** By identifying layers where DANN diverges most from ERM (via CKA), we can \"transplant\" domain-invariant representations into ERM at inference time.\n",
        "\n",
        "## Approach\n",
        "\n",
        "1. **CKA Analysis** — Identify which layers show largest representation difference between ERM and DANN\n",
        "2. **Activation Patching** — During inference, replace ERM activations at layer L with DANN activations\n",
        "3. **Evaluation** — Measure domain probe accuracy and EER on patched models\n",
        "\n",
        "## Expected Outcome\n",
        "\n",
        "If successful, this provides a lightweight method to improve domain robustness without expensive DANN training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path(\"..\").resolve()\n",
        "src_root = project_root / \"src\"\n",
        "for candidate_path in (src_root, project_root):\n",
        "    candidate_path_str = str(candidate_path)\n",
        "    if candidate_path_str not in sys.path:\n",
        "        sys.path.insert(0, candidate_path_str)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Core utilities\n",
        "from asvspoof5_domain_invariant_cm.utils import (\n",
        "    get_device,\n",
        "    get_manifest_path,\n",
        "    get_manifests_dir,\n",
        "    get_runs_dir,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# Data loading - use create_dataloader (not get_dataloader)\n",
        "from asvspoof5_domain_invariant_cm.data import (\n",
        "    ASVspoof5Dataset,\n",
        "    AudioCollator,\n",
        "    create_dataloader,\n",
        "    load_vocab,\n",
        ")\n",
        "\n",
        "# Analysis tools\n",
        "from asvspoof5_domain_invariant_cm.analysis import (\n",
        "    compute_linear_cka,\n",
        "    compare_representations,\n",
        "    layerwise_probing,\n",
        "    ActivationCache,\n",
        "    register_hooks,\n",
        "    remove_hooks,\n",
        ")\n",
        "\n",
        "# Evaluation metrics\n",
        "from asvspoof5_domain_invariant_cm.evaluation import compute_eer, compute_min_dcf\n",
        "\n",
        "# Model components\n",
        "from asvspoof5_domain_invariant_cm.models import (\n",
        "    ClassifierHead,\n",
        "    DANNModel,\n",
        "    ERMModel,\n",
        "    MultiHeadDomainDiscriminator,\n",
        "    ProjectionHead,\n",
        "    create_backbone,\n",
        "    create_pooling,\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Checkpoints\n",
        "\n",
        "Load the ERM and DANN models (WavLM backbone).\n",
        "\n",
        "We use a robust model loading function that handles architecture reconstruction from config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(checkpoint_path: Path, device: torch.device):\n",
        "    \"\"\"Load model from checkpoint with proper architecture reconstruction.\n",
        "    \n",
        "    Adapted from scripts/evaluate.py - handles both ERM and DANN models,\n",
        "    auto-detects discriminator dimensions from weights.\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint file.\n",
        "        device: Device to load the model onto.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (model, config, codec_vocab, codec_q_vocab).\n",
        "    \"\"\"\n",
        "    if not checkpoint_path.exists():\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    config = checkpoint.get(\"config\", {})\n",
        "    state_dict = checkpoint.get(\"model_state_dict\", {})\n",
        "\n",
        "    run_dir = checkpoint_path.parent.parent\n",
        "    run_codec_vocab_path = run_dir / \"codec_vocab.json\"\n",
        "    run_codec_q_vocab_path = run_dir / \"codec_q_vocab.json\"\n",
        "    fallback_codec_vocab_path = get_manifests_dir() / \"codec_vocab.json\"\n",
        "    fallback_codec_q_vocab_path = get_manifests_dir() / \"codec_q_vocab.json\"\n",
        "\n",
        "    codec_vocab_path = run_codec_vocab_path if run_codec_vocab_path.exists() else fallback_codec_vocab_path\n",
        "    codec_q_vocab_path = run_codec_q_vocab_path if run_codec_q_vocab_path.exists() else fallback_codec_q_vocab_path\n",
        "\n",
        "    if not codec_vocab_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing codec_vocab.json. Checked {run_codec_vocab_path} and {fallback_codec_vocab_path}.\"\n",
        "        )\n",
        "    if not codec_q_vocab_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing codec_q_vocab.json. Checked {run_codec_q_vocab_path} and {fallback_codec_q_vocab_path}.\"\n",
        "        )\n",
        "\n",
        "    codec_vocab = load_vocab(codec_vocab_path)\n",
        "    codec_q_vocab = load_vocab(codec_q_vocab_path)\n",
        "\n",
        "    # Build architecture from config\n",
        "    backbone_cfg = config.get(\"backbone\", {})\n",
        "    projection_cfg = config.get(\"projection\", {})\n",
        "    classifier_cfg = config.get(\"classifier\", {})\n",
        "    pooling_cfg = config.get(\"pooling\", {})\n",
        "    training_cfg = config.get(\"training\", {})\n",
        "\n",
        "    layer_selection = backbone_cfg.get(\"layer_selection\", {})\n",
        "    backbone = create_backbone(\n",
        "        name=backbone_cfg.get(\"name\", \"wavlm_base_plus\"),\n",
        "        pretrained=backbone_cfg.get(\"pretrained\", \"microsoft/wavlm-base-plus\"),\n",
        "        freeze=True,  # Always freeze for inference\n",
        "        layer_selection=layer_selection.get(\"method\", \"weighted\"),\n",
        "        k=layer_selection.get(\"k\", 6),\n",
        "        layer_indices=layer_selection.get(\"layers\"),\n",
        "        init_lower_bias=layer_selection.get(\"init_lower_bias\", True),\n",
        "    )\n",
        "\n",
        "    pooling_method = pooling_cfg.get(\"method\", \"stats\")\n",
        "    pooling = create_pooling(pooling_method, backbone.hidden_size)\n",
        "\n",
        "    proj_input_dim = backbone.hidden_size * 2 if pooling_method == \"stats\" else backbone.hidden_size\n",
        "\n",
        "    projection = ProjectionHead(\n",
        "        input_dim=proj_input_dim,\n",
        "        hidden_dim=projection_cfg.get(\"hidden_dim\", 512),\n",
        "        output_dim=projection_cfg.get(\"output_dim\", 256),\n",
        "        num_layers=projection_cfg.get(\"num_layers\", 2),\n",
        "        dropout=projection_cfg.get(\"dropout\", 0.1),\n",
        "    )\n",
        "\n",
        "    repr_dim = projection_cfg.get(\"output_dim\", 256)\n",
        "    task_head = ClassifierHead(\n",
        "        input_dim=repr_dim,\n",
        "        num_classes=classifier_cfg.get(\"num_classes\", 2),\n",
        "        hidden_dim=classifier_cfg.get(\"hidden_dim\", 0),\n",
        "        dropout=classifier_cfg.get(\"dropout\", 0.1),\n",
        "    )\n",
        "\n",
        "    method = training_cfg.get(\"method\", \"erm\")\n",
        "\n",
        "    if method == \"dann\":\n",
        "        dann_cfg = config.get(\"dann\", {})\n",
        "        disc_cfg = dann_cfg.get(\"discriminator\", {})\n",
        "        \n",
        "        # Auto-detect from weights if available\n",
        "        disc_weight_key = \"domain_discriminator.shared.0.weight\"\n",
        "        if disc_weight_key in state_dict:\n",
        "            disc_input_dim = state_dict[disc_weight_key].shape[1]\n",
        "        else:\n",
        "            disc_input_dim = disc_cfg.get(\"input_dim\", proj_input_dim)\n",
        "\n",
        "        domain_discriminator = MultiHeadDomainDiscriminator(\n",
        "            input_dim=disc_input_dim,\n",
        "            num_codecs=len(codec_vocab),\n",
        "            num_codec_qs=len(codec_q_vocab),\n",
        "            hidden_dim=disc_cfg.get(\"hidden_dim\", 512),\n",
        "            dropout=disc_cfg.get(\"dropout\", 0.1),\n",
        "        )\n",
        "\n",
        "        model = DANNModel(\n",
        "            backbone=backbone,\n",
        "            pooling=pooling,\n",
        "            projection=projection,\n",
        "            task_head=task_head,\n",
        "            domain_discriminator=domain_discriminator,\n",
        "            lambda_=0.0,  # No GRL during inference\n",
        "        )\n",
        "    else:\n",
        "        model = ERMModel(\n",
        "            backbone=backbone,\n",
        "            pooling=pooling,\n",
        "            projection=projection,\n",
        "            task_head=task_head,\n",
        "        )\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, config, codec_vocab, codec_q_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERM checkpoint: /Users/jmqcooper/Documents/Development/asvspoof5-domain-invariant-cm/runs/wavlm_erm/checkpoints/best.pt\n",
            "DANN checkpoint: /Users/jmqcooper/Documents/Development/asvspoof5-domain-invariant-cm/runs/wavlm_dann/checkpoints/epoch_5_patched.pt\n",
            "ERM checkpoint exists: False\n",
            "DANN checkpoint exists: False\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Missing ERM/DANN checkpoints. Set RUNS_DIR or place checkpoints under runs/{wavlm_erm,wavlm_dann}/checkpoints/.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDANN checkpoint exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDANN_CHECKPOINT.exists()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ERM_CHECKPOINT.exists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DANN_CHECKPOINT.exists():\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMissing ERM/DANN checkpoints. Set RUNS_DIR or place checkpoints under runs/\u001b[39m\u001b[33m{\u001b[39m\u001b[33mwavlm_erm,wavlm_dann}/checkpoints/.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading ERM model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Missing ERM/DANN checkpoints. Set RUNS_DIR or place checkpoints under runs/{wavlm_erm,wavlm_dann}/checkpoints/."
          ]
        }
      ],
      "source": [
        "def resolve_checkpoint_path(*relative_parts: str) -> Path:\n",
        "    \"\"\"Resolve checkpoints from RUNS_DIR env or project runs directory.\"\"\"\n",
        "    runs_dir = Path(os.environ[\"RUNS_DIR\"]) if \"RUNS_DIR\" in os.environ else get_runs_dir()\n",
        "    return runs_dir.joinpath(*relative_parts)\n",
        "\n",
        "\n",
        "# Parameterized pairwise setup (defaults keep prior ERM vs DANN behavior).\n",
        "model_a_run = os.environ.get(\"RQ4_MODEL_A_RUN\", \"wavlm_erm\")\n",
        "model_a_ckpt = os.environ.get(\"RQ4_MODEL_A_CKPT\", \"best.pt\")\n",
        "model_b_run = os.environ.get(\"RQ4_MODEL_B_RUN\", \"wavlm_dann\")\n",
        "model_b_ckpt = os.environ.get(\"RQ4_MODEL_B_CKPT\", \"epoch_5_patched.pt\")\n",
        "\n",
        "MODEL_A_CHECKPOINT = resolve_checkpoint_path(model_a_run, \"checkpoints\", model_a_ckpt)\n",
        "MODEL_B_CHECKPOINT = resolve_checkpoint_path(model_b_run, \"checkpoints\", model_b_ckpt)\n",
        "\n",
        "print(f\"Model A checkpoint: {MODEL_A_CHECKPOINT}\")\n",
        "print(f\"Model B checkpoint: {MODEL_B_CHECKPOINT}\")\n",
        "print(f\"Model A checkpoint exists: {MODEL_A_CHECKPOINT.exists()}\")\n",
        "print(f\"Model B checkpoint exists: {MODEL_B_CHECKPOINT.exists()}\")\n",
        "\n",
        "if not MODEL_A_CHECKPOINT.exists() or not MODEL_B_CHECKPOINT.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Missing model checkpoints. Configure RUNS_DIR and optionally \"\n",
        "        \"RQ4_MODEL_A_RUN/RQ4_MODEL_A_CKPT/RQ4_MODEL_B_RUN/RQ4_MODEL_B_CKPT.\"\n",
        "    )\n",
        "\n",
        "# Load models\n",
        "print(\"\\nLoading model A...\")\n",
        "model_a_model, model_a_config, codec_vocab, codec_q_vocab = load_model_from_checkpoint(MODEL_A_CHECKPOINT, device)\n",
        "model_a_method = model_a_config.get(\"training\", {}).get(\"method\", \"erm\")\n",
        "print(f\"Model A method: {model_a_method}\")\n",
        "\n",
        "print(\"\\nLoading model B...\")\n",
        "model_b_model, model_b_config, _, _ = load_model_from_checkpoint(MODEL_B_CHECKPOINT, device)\n",
        "model_b_method = model_b_config.get(\"training\", {}).get(\"method\", \"dann\")\n",
        "print(f\"Model B method: {model_b_method}\")\n",
        "\n",
        "supported_methods = {\"erm\", \"dann\"}\n",
        "if model_a_method not in supported_methods or model_b_method not in supported_methods:\n",
        "    raise NotImplementedError(\n",
        "        \"This notebook currently supports training.method in {'erm', 'dann'} only. \"\n",
        "        f\"Got model_a={model_a_method}, model_b={model_b_method}.\"\n",
        "    )\n",
        "\n",
        "model_a_name = os.environ.get(\"RQ4_MODEL_A_LABEL\", f\"A:{model_a_method.upper()}\")\n",
        "model_b_name = os.environ.get(\"RQ4_MODEL_B_LABEL\", f\"B:{model_b_method.upper()}\")\n",
        "patched_model_name = f\"Patched {model_a_name}\"\n",
        "\n",
        "print(f\"\\nModel labels: {model_a_name} vs {model_b_name}\")\n",
        "print(\"DANN domain objective is applied on pre-projection pooled features.\")\n",
        "print(\"With a frozen backbone, strongest differences are expected after backbone hidden states.\")\n",
        "\n",
        "# Backward-compatible aliases for existing downstream variable names.\n",
        "erm_model, erm_config = model_a_model, model_a_config\n",
        "dann_model, dann_config = model_b_model, model_b_config\n",
        "\n",
        "print(f\"\\nCodec vocab size: {len(codec_vocab)}\")\n",
        "print(f\"Codec Q vocab size: {len(codec_q_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CKA Analysis\n",
        "\n",
        "Centered Kernel Alignment (CKA) measures representational similarity between layers.\n",
        "We compute CKA between ERM and DANN at each layer to identify where they diverge most.\n",
        "\n",
        "The library's `compute_linear_cka` function already handles numerical stability with epsilon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_eval_dataloader(\n",
        "    split: str = \"dev\",\n",
        "    codec_vocab: dict = None,\n",
        "    codec_q_vocab: dict = None,\n",
        "    config: dict = None,\n",
        "    max_samples: int = None,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 4,\n",
        "    seed: int = 42,\n",
        ") -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Create dataloader for CKA analysis and evaluation.\n",
        "    \n",
        "    Args:\n",
        "        split: Data split ('dev' or 'eval').\n",
        "        codec_vocab: CODEC vocabulary.\n",
        "        codec_q_vocab: CODEC_Q vocabulary.\n",
        "        config: Model config dict.\n",
        "        max_samples: Maximum samples to use (None for all).\n",
        "        batch_size: Batch size.\n",
        "        num_workers: Number of workers.\n",
        "        seed: Random seed for subset selection.\n",
        "    \n",
        "    Returns:\n",
        "        DataLoader instance.\n",
        "    \"\"\"\n",
        "    audio_cfg = config.get(\"audio\", {}) if config else {}\n",
        "    sample_rate = audio_cfg.get(\"sample_rate\", 16000)\n",
        "    max_duration = audio_cfg.get(\"max_duration_sec\", 6.0)\n",
        "    \n",
        "    manifest_path = get_manifest_path(split)\n",
        "    print(f\"Loading manifest from: {manifest_path}\")\n",
        "    if not manifest_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing manifest: {manifest_path}. Run scripts/prepare_asvspoof5.py after setting ASVSPOOF5_ROOT.\"\n",
        "        )\n",
        "\n",
        "    dataset = ASVspoof5Dataset(\n",
        "        manifest_path=manifest_path,\n",
        "        codec_vocab=codec_vocab,\n",
        "        codec_q_vocab=codec_q_vocab,\n",
        "        max_duration_sec=max_duration,\n",
        "        sample_rate=sample_rate,\n",
        "        mode=\"eval\",\n",
        "    )\n",
        "    \n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    \n",
        "    # Subsample if needed\n",
        "    if max_samples and max_samples < len(dataset):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        indices = rng.choice(len(dataset), size=max_samples, replace=False)\n",
        "        dataset = torch.utils.data.Subset(dataset, indices)\n",
        "        print(f\"Subsampled to {len(dataset)} samples\")\n",
        "    \n",
        "    fixed_length = int(max_duration * sample_rate)\n",
        "    collator = AudioCollator(fixed_length=fixed_length, mode=\"eval\")\n",
        "    \n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collator,\n",
        "        pin_memory=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_selected_layer_indices(backbone: torch.nn.Module, total_layers: int) -> list[int]:\n",
        "    \"\"\"Resolve which hidden-state indices participate in backbone layer mixing.\"\"\"\n",
        "    selection = getattr(backbone, \"layer_selection\", \"weighted\")\n",
        "    k = int(getattr(backbone, \"k\", total_layers))\n",
        "    explicit_indices = getattr(backbone, \"layer_indices\", None)\n",
        "\n",
        "    if selection == \"first_k\":\n",
        "        return list(range(min(k, total_layers)))\n",
        "    if selection == \"last_k\":\n",
        "        start = max(0, total_layers - k)\n",
        "        return list(range(start, total_layers))\n",
        "    if selection == \"specific\" and explicit_indices:\n",
        "        return [int(idx) for idx in explicit_indices if 0 <= int(idx) < total_layers]\n",
        "\n",
        "    # weighted / fallback: use all transformer layers\n",
        "    return list(range(total_layers))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_layer_representations(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "    num_batches: int = None,\n",
        "    representation: str = \"layer_contrib\",\n",
        ") -> dict:\n",
        "    \"\"\"Extract model representations for CKA.\n",
        "\n",
        "    Supported representations: hidden_states, mixed, repr, layer_contrib.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    layer_reps = {}\n",
        "    total_batches = num_batches if num_batches else len(dataloader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloader, total=total_batches, desc=f\"Extracting ({representation})\")):\n",
        "        if num_batches and batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        waveform = batch[\"waveform\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        lengths = batch[\"lengths\"].to(device)\n",
        "\n",
        "        outputs = model(waveform, attention_mask, lengths)\n",
        "        all_hidden_states = outputs.get(\"all_hidden_states\", [])\n",
        "        if not all_hidden_states:\n",
        "            raise RuntimeError(\"Model did not return all_hidden_states. Check model forward method.\")\n",
        "\n",
        "        if representation == \"hidden_states\":\n",
        "            for layer_idx, hidden_state in enumerate(all_hidden_states):\n",
        "                pooled = hidden_state.mean(dim=1)\n",
        "                layer_reps.setdefault(layer_idx, []).append(pooled.cpu())\n",
        "\n",
        "        elif representation == \"repr\":\n",
        "            if \"repr\" not in outputs:\n",
        "                raise RuntimeError(\"Model did not return repr. Cannot compute CKA with representation='repr'.\")\n",
        "            layer_reps.setdefault(\"repr\", []).append(outputs[\"repr\"].cpu())\n",
        "\n",
        "        elif representation == \"mixed\":\n",
        "            mixed, _ = model.backbone(waveform, attention_mask)\n",
        "            layer_reps.setdefault(\"mixed\", []).append(mixed.mean(dim=1).cpu())\n",
        "\n",
        "        elif representation == \"layer_contrib\":\n",
        "            total_layers = len(all_hidden_states)\n",
        "            selected_indices = _get_selected_layer_indices(model.backbone, total_layers)\n",
        "            selected_states = [all_hidden_states[idx] for idx in selected_indices]\n",
        "\n",
        "            layer_pooling = getattr(model.backbone, \"layer_pooling\", None)\n",
        "            if layer_pooling is None or not hasattr(layer_pooling, \"weights\"):\n",
        "                raise RuntimeError(\"Backbone missing layer_pooling.weights needed for layer_contrib extraction.\")\n",
        "\n",
        "            weights = torch.softmax(layer_pooling.weights.detach(), dim=0)\n",
        "            if weights.numel() != len(selected_states):\n",
        "                raise RuntimeError(\n",
        "                    \"Layer weight count does not match selected states: \"\n",
        "                    f\"weights={weights.numel()} states={len(selected_states)}\"\n",
        "                )\n",
        "\n",
        "            for local_idx, (layer_idx, hidden_state) in enumerate(zip(selected_indices, selected_states)):\n",
        "                contribution = hidden_state * weights[local_idx]\n",
        "                pooled = contribution.mean(dim=1)\n",
        "                layer_reps.setdefault(int(layer_idx), []).append(pooled.cpu())\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unknown representation '{representation}'. \"\n",
        "                \"Expected one of: hidden_states, mixed, repr, layer_contrib\"\n",
        "            )\n",
        "\n",
        "    result = {}\n",
        "    for key, rep_list in layer_reps.items():\n",
        "        if rep_list:\n",
        "            result[key] = torch.cat(rep_list, dim=0)\n",
        "\n",
        "    print(f\"Extracted representations for {len(result)} keys: {sorted(result.keys(), key=lambda x: str(x))}\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select representation used by CKA and downstream patch-layer selection.\n",
        "# Use layer_contrib by default; hidden_states is usually trivial (CKA~1.0) with frozen backbones.\n",
        "cka_representation = \"layer_contrib\"\n",
        "valid_representations = {\"hidden_states\", \"mixed\", \"repr\", \"layer_contrib\"}\n",
        "if cka_representation not in valid_representations:\n",
        "    raise ValueError(f\"Invalid cka_representation={cka_representation}. Choose from {sorted(valid_representations)}\")\n",
        "\n",
        "if cka_representation == \"hidden_states\":\n",
        "    is_frozen_base = bool(erm_config.get(\"backbone\", {}).get(\"freeze\", True))\n",
        "    is_frozen_donor = bool(dann_config.get(\"backbone\", {}).get(\"freeze\", True))\n",
        "    if is_frozen_base and is_frozen_donor:\n",
        "        print(\n",
        "            \"Warning: hidden_states with frozen SSL backbones typically yields CKA~1.0. \"\n",
        "            \"Use 'layer_contrib', 'mixed', or 'repr' for a more informative comparison.\"\n",
        "        )\n",
        "\n",
        "# Create dataloader for CKA analysis (use subset for speed)\n",
        "print(f\"Creating evaluation dataloader for CKA analysis ({cka_representation})...\")\n",
        "eval_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=erm_config,\n",
        "    max_samples=5000,  # Use subset for speed\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract representations from model A\n",
        "print(f\"Extracting {model_a_name} representations ({cka_representation})...\")\n",
        "erm_reps = extract_layer_representations(\n",
        "    erm_model,\n",
        "    eval_loader,\n",
        "    device,\n",
        "    representation=cka_representation,\n",
        ")\n",
        "print(f\"Extracted {len(erm_reps)} keys, shapes: {[(k, v.shape) for k, v in erm_reps.items()]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset dataloader for model B\n",
        "eval_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=dann_config,\n",
        "    max_samples=5000,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "# Extract representations from model B\n",
        "print(f\"Extracting {model_b_name} representations ({cka_representation})...\")\n",
        "dann_reps = extract_layer_representations(\n",
        "    dann_model,\n",
        "    eval_loader,\n",
        "    device,\n",
        "    representation=cka_representation,\n",
        ")\n",
        "print(f\"Extracted {len(dann_reps)} keys, shapes: {[(k, v.shape) for k, v in dann_reps.items()]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute CKA per representation key\n",
        "print(f\"Computing CKA between {model_a_name} and {model_b_name} using '{cka_representation}'...\")\n",
        "cka_results = compare_representations(\n",
        "    erm_layers={k: v.numpy() for k, v in erm_reps.items()},\n",
        "    dann_layers={k: v.numpy() for k, v in dann_reps.items()},\n",
        ")\n",
        "\n",
        "print(\"\\nCKA Results:\")\n",
        "print(f\"Model pair: {model_a_name} vs {model_b_name}\")\n",
        "print(f\"Representation mode: {cka_representation}\")\n",
        "print(f\"Mean CKA: {cka_results['mean_cka']:.4f}\")\n",
        "print(f\"Min CKA:  {cka_results['min_cka']:.4f}\")\n",
        "print(f\"Max CKA:  {cka_results['max_cka']:.4f}\")\n",
        "print(f\"Most different key: {cka_results['most_different_layer']}\")\n",
        "\n",
        "print(\"\\nPer-key CKA:\")\n",
        "for key in sorted(cka_results['per_layer'].keys(), key=lambda x: str(x)):\n",
        "    cka = cka_results['per_layer'][key]['cka']\n",
        "    print(f\"  Key {key}: CKA = {cka:.4f}\")\n",
        "\n",
        "cka_layer_keys = [k for k in cka_results['per_layer'].keys() if isinstance(k, int)]\n",
        "if len(cka_layer_keys) < 3:\n",
        "    raise RuntimeError(\n",
        "        \"Activation patching needs at least 3 integer transformer-layer keys from CKA. \"\n",
        "        f\"Current keys: {sorted(cka_results['per_layer'].keys(), key=lambda x: str(x))}. \"\n",
        "        \"Use cka_representation='hidden_states' or 'layer_contrib'.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot CKA scores for integer layer keys (needed for activation patching)\n",
        "layers = sorted([k for k in cka_results['per_layer'].keys() if isinstance(k, int)])\n",
        "if len(layers) < 3:\n",
        "    raise RuntimeError(\n",
        "        \"Cannot plot patch-layer CKA bars because fewer than 3 integer layer keys were found. \"\n",
        "        \"Use cka_representation='hidden_states' or 'layer_contrib'.\"\n",
        "    )\n",
        "\n",
        "cka_scores = [cka_results['per_layer'][l]['cka'] for l in layers]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "bars = plt.bar(layers, cka_scores, color='steelblue', edgecolor='black')\n",
        "\n",
        "# Highlight the most divergent layers\n",
        "sorted_by_cka = sorted(layers, key=lambda l: cka_results['per_layer'][l]['cka'])\n",
        "divergent_layers = sorted_by_cka[:3]\n",
        "for i, l in enumerate(layers):\n",
        "    if l in divergent_layers:\n",
        "        bars[i].set_color('tomato')\n",
        "\n",
        "plt.xlabel('Layer Index', fontsize=12)\n",
        "plt.ylabel('CKA Similarity', fontsize=12)\n",
        "plt.title(\n",
        "    f\"{model_a_name} vs {model_b_name} Representation Similarity (CKA, {cka_representation})\\nRed = Most Divergent Layers\",\n",
        "    fontsize=14,\n",
        ")\n",
        "plt.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='High similarity threshold')\n",
        "plt.ylim(0, 1.05)\n",
        "plt.xticks(layers)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMost divergent layers (lowest CKA): {divergent_layers}\")\n",
        "cka_value_strings = [f\"{cka_results['per_layer'][l]['cka']:.4f}\" for l in divergent_layers]\n",
        "print(f\"CKA values: {cka_value_strings}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Activation Patching\n",
        "\n",
        "Replace ERM activations at specific layers with DANN activations during inference.\n",
        "\n",
        "We use forward hooks to:\n",
        "1. Capture DANN activations at target layers\n",
        "2. Replace ERM activations with DANN activations at those layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchedModel(torch.nn.Module):\n",
        "    \"\"\"Model that patches activations from a donor model at specified layers.\n",
        "\n",
        "    Hooks into the backbone's internal transformer layers to replace\n",
        "    hidden states from model_a with those from model_b.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model: torch.nn.Module,\n",
        "        donor_model: torch.nn.Module,\n",
        "        patch_layers: list[int],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_model: The model to run inference on.\n",
        "            donor_model: The model to take activations from.\n",
        "            patch_layers: List of layer indices to patch (0-indexed transformer layers).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.donor_model = donor_model\n",
        "        self.patch_layers = set(patch_layers)\n",
        "\n",
        "        # Keep deterministic execution while patching.\n",
        "        self.base_model.eval()\n",
        "        self.donor_model.eval()\n",
        "\n",
        "        self._donor_activations = {}\n",
        "        self._handles = []\n",
        "        self._setup_hooks()\n",
        "\n",
        "    def _get_layer_name(self, layer_idx: int) -> str:\n",
        "        \"\"\"Get the full module name for a transformer layer.\n",
        "\n",
        "        For WavLM: backbone.model.encoder.layers.{layer_idx}\n",
        "        Note: layer_idx 0 corresponds to the first transformer layer (after CNN encoder)\n",
        "        \"\"\"\n",
        "        return f\"backbone.model.encoder.layers.{layer_idx}\"\n",
        "\n",
        "    def _setup_hooks(self):\n",
        "        \"\"\"Setup forward hooks to capture and replace activations.\"\"\"\n",
        "        for layer_idx in self.patch_layers:\n",
        "            layer_name = self._get_layer_name(layer_idx)\n",
        "\n",
        "            # Find module in donor\n",
        "            donor_module = dict(self.donor_model.named_modules()).get(layer_name)\n",
        "            if donor_module is None:\n",
        "                raise ValueError(f\"Layer {layer_name} not found in donor model\")\n",
        "\n",
        "            def make_capture_hook(idx):\n",
        "                def hook(module, input, output):\n",
        "                    if isinstance(output, tuple):\n",
        "                        self._donor_activations[idx] = output[0].detach()\n",
        "                    else:\n",
        "                        self._donor_activations[idx] = output.detach()\n",
        "\n",
        "                return hook\n",
        "\n",
        "            handle = donor_module.register_forward_hook(make_capture_hook(layer_idx))\n",
        "            self._handles.append(handle)\n",
        "\n",
        "        # Register patching hooks on base model\n",
        "        for layer_idx in self.patch_layers:\n",
        "            layer_name = self._get_layer_name(layer_idx)\n",
        "\n",
        "            base_module = dict(self.base_model.named_modules()).get(layer_name)\n",
        "            if base_module is None:\n",
        "                raise ValueError(f\"Layer {layer_name} not found in base model\")\n",
        "\n",
        "            def make_patch_hook(idx):\n",
        "                def hook(module, input, output):\n",
        "                    if idx not in self._donor_activations:\n",
        "                        return output\n",
        "\n",
        "                    donor_act = self._donor_activations[idx]\n",
        "                    base_hidden = output[0] if isinstance(output, tuple) else output\n",
        "                    if donor_act.shape != base_hidden.shape:\n",
        "                        raise RuntimeError(\n",
        "                            \"Activation shape mismatch during patching at layer \"\n",
        "                            f\"{idx}: donor={tuple(donor_act.shape)} vs base={tuple(base_hidden.shape)}\"\n",
        "                        )\n",
        "\n",
        "                    if isinstance(output, tuple):\n",
        "                        # WavLM carries position_bias in tuple index 1. Reset it to force\n",
        "                        # recomputation with donor-shaped hidden states in the next layer.\n",
        "                        if len(output) >= 2:\n",
        "                            return (donor_act, None, *output[2:])\n",
        "                        return (donor_act,)\n",
        "                    return donor_act\n",
        "\n",
        "                return hook\n",
        "\n",
        "            handle = base_module.register_forward_hook(make_patch_hook(layer_idx))\n",
        "            self._handles.append(handle)\n",
        "\n",
        "        print(f\"Registered {len(self._handles)} hooks for patching layers {sorted(self.patch_layers)}\")\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Remove all registered hooks.\"\"\"\n",
        "        for handle in self._handles:\n",
        "            handle.remove()\n",
        "        self._handles = []\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        waveform: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None,\n",
        "        lengths: torch.Tensor = None,\n",
        "    ) -> dict:\n",
        "        \"\"\"Forward pass with activation patching.\n",
        "\n",
        "        1. Run donor model to capture activations\n",
        "        2. Run base model with patched activations (via hooks)\n",
        "        \"\"\"\n",
        "        self._donor_activations.clear()\n",
        "\n",
        "        # Run donor to capture activations\n",
        "        with torch.no_grad():\n",
        "            _ = self.donor_model(waveform, attention_mask, lengths)\n",
        "\n",
        "        missing_layers = sorted(self.patch_layers.difference(self._donor_activations.keys()))\n",
        "        if missing_layers:\n",
        "            raise RuntimeError(f\"Missing donor activations for patch layers: {missing_layers}\")\n",
        "\n",
        "        # Run base model (hooks will patch activations)\n",
        "        output = self.base_model(waveform, attention_mask, lengths)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup hooks on deletion.\"\"\"\n",
        "        self.remove_hooks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify most divergent layers (lowest CKA = most different representations)\n",
        "cka_per_layer = cka_results['per_layer']\n",
        "sorted_layers = sorted(\n",
        "    [k for k in cka_per_layer.keys() if isinstance(k, int)],\n",
        "    key=lambda k: cka_per_layer[k]['cka'],\n",
        ")\n",
        "\n",
        "# Patch the 3 most divergent transformer layers that exist in the model\n",
        "available_transformer_layers = {\n",
        "    int(name.split(\".\")[-1])\n",
        "    for name, _ in erm_model.named_modules()\n",
        "    if name.startswith(\"backbone.model.encoder.layers.\") and name.split(\".\")[-1].isdigit()\n",
        "}\n",
        "divergent_layers = [layer_idx for layer_idx in sorted_layers if layer_idx in available_transformer_layers][:3]\n",
        "if len(divergent_layers) < 3:\n",
        "    raise RuntimeError(\n",
        "        f\"Expected at least 3 patchable transformer layers, found {len(divergent_layers)}. \"\n",
        "        f\"Available layers: {sorted(available_transformer_layers)}\"\n",
        "    )\n",
        "print(f\"Most divergent transformer layers (lowest CKA): {divergent_layers}\")\n",
        "cka_value_strings = [f\"{cka_per_layer[layer_idx]['cka']:.4f}\" for layer_idx in divergent_layers]\n",
        "print(f\"CKA values: {cka_value_strings}\")\n",
        "\n",
        "# Create patched model (base=model A, donor=model B)\n",
        "patched_model = PatchedModel(\n",
        "    base_model=erm_model,\n",
        "    donor_model=dann_model,\n",
        "    patch_layers=divergent_layers,\n",
        ")\n",
        "patched_model.eval()\n",
        "print(f\"\\n{patched_model_name} created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation\n",
        "\n",
        "Compare:\n",
        "1. Original ERM\n",
        "2. Original DANN\n",
        "3. Patched ERM (with DANN activations at selected layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        ") -> dict:\n",
        "    \"\"\"Compute EER and collect predictions for analysis.\n",
        "    \n",
        "    Args:\n",
        "        model: Model to evaluate.\n",
        "        dataloader: Evaluation dataloader.\n",
        "        device: Computation device.\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'eer', 'min_dcf', 'scores', 'labels'\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "    \n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        waveform = batch['waveform'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        lengths = batch['lengths'].to(device)\n",
        "        # Defensive access - handle case where y_task might not be present\n",
        "        labels = batch.get('y_task')\n",
        "        if labels is None:\n",
        "            raise ValueError(\n",
        "                \"Batch missing 'y_task' field. Ensure dataloader/collator \"\n",
        "                \"returns task labels in eval mode.\"\n",
        "            )\n",
        "        \n",
        "        outputs = model(waveform, attention_mask, lengths)\n",
        "        \n",
        "        # Score convention in this codebase: higher score = more likely bonafide\n",
        "        # Labels: 0 = bonafide, 1 = spoof\n",
        "        probs = torch.softmax(outputs['task_logits'], dim=-1)\n",
        "        scores = probs[:, 0].cpu().numpy()  # P(bonafide)\n",
        "        \n",
        "        all_scores.extend(scores)\n",
        "        all_labels.extend(labels.numpy())\n",
        "    \n",
        "    scores = np.array(all_scores)\n",
        "    labels = np.array(all_labels)\n",
        "    \n",
        "    eer, threshold = compute_eer(scores, labels)\n",
        "    min_dcf = compute_min_dcf(scores, labels)\n",
        "    \n",
        "    return {\n",
        "        'eer': eer,\n",
        "        'min_dcf': min_dcf,\n",
        "        'eer_threshold': threshold,\n",
        "        'scores': scores,\n",
        "        'labels': labels,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fresh dataloader for evaluation (full dev set or subset)\n",
        "print(\"Creating evaluation dataloader...\")\n",
        "eval_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=erm_config,\n",
        "    max_samples=10000,  # Use subset for faster evaluation, None for full set\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model A\n",
        "print(f\"Evaluating {model_a_name}...\")\n",
        "erm_results = evaluate_model(erm_model, eval_loader, device)\n",
        "print(f\"{model_a_name} EER: {erm_results['eer']:.2%}, minDCF: {erm_results['min_dcf']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate dataloader for DANN\n",
        "eval_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=dann_config,\n",
        "    max_samples=10000,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "# Evaluate model B\n",
        "print(f\"Evaluating {model_b_name}...\")\n",
        "dann_results = evaluate_model(dann_model, eval_loader, device)\n",
        "print(f\"{model_b_name} EER: {dann_results['eer']:.2%}, minDCF: {dann_results['min_dcf']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate dataloader for patched model\n",
        "eval_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=erm_config,\n",
        "    max_samples=10000,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "# Evaluate patched model A\n",
        "print(f\"Evaluating {patched_model_name}...\")\n",
        "patched_results = evaluate_model(patched_model, eval_loader, device)\n",
        "print(f\"{patched_model_name} EER: {patched_results['eer']:.2%}, minDCF: {patched_results['min_dcf']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<20} {'EER':>10} {'minDCF':>10}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{model_a_name:<20} {erm_results['eer']:>9.2%} {erm_results['min_dcf']:>10.4f}\")\n",
        "print(f\"{model_b_name:<20} {dann_results['eer']:>9.2%} {dann_results['min_dcf']:>10.4f}\")\n",
        "print(f\"{patched_model_name:<20} {patched_results['eer']:>9.2%} {patched_results['min_dcf']:>10.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Domain Probe on Patched Model\n",
        "\n",
        "Verify that patching reduces domain leakage by running codec probes on the patched representations.\n",
        "\n",
        "Lower probe accuracy = more domain-invariant representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_representations_for_probing(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "    max_samples: int = 5000,\n",
        "    representation: str = \"hidden_states\",\n",
        ") -> tuple[dict, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Extract representations and domain labels for probing.\n",
        "\n",
        "    Supported representations: hidden_states, mixed, repr, layer_contrib.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    layer_reps = {}\n",
        "    all_codec = []\n",
        "    all_codec_q = []\n",
        "    n_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Extracting for probing ({representation})\"):\n",
        "            if max_samples and n_samples >= max_samples:\n",
        "                break\n",
        "\n",
        "            waveform = batch[\"waveform\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            lengths = batch[\"lengths\"].to(device)\n",
        "\n",
        "            outputs = model(waveform, attention_mask, lengths)\n",
        "            all_hidden_states = outputs.get(\"all_hidden_states\", [])\n",
        "            batch_size = waveform.shape[0]\n",
        "\n",
        "            if representation == \"hidden_states\":\n",
        "                for layer_idx, hidden_state in enumerate(all_hidden_states):\n",
        "                    pooled = hidden_state.mean(dim=1).cpu().numpy()\n",
        "                    layer_reps.setdefault(layer_idx, []).append(pooled)\n",
        "\n",
        "            elif representation == \"repr\":\n",
        "                if \"repr\" not in outputs:\n",
        "                    raise RuntimeError(\"Model did not return repr. Cannot probe representation='repr'.\")\n",
        "                layer_reps.setdefault(\"repr\", []).append(outputs[\"repr\"].cpu().numpy())\n",
        "\n",
        "            elif representation == \"mixed\":\n",
        "                mixed, _ = model.backbone(waveform, attention_mask)\n",
        "                layer_reps.setdefault(\"mixed\", []).append(mixed.mean(dim=1).cpu().numpy())\n",
        "\n",
        "            elif representation == \"layer_contrib\":\n",
        "                total_layers = len(all_hidden_states)\n",
        "                selected_indices = _get_selected_layer_indices(model.backbone, total_layers)\n",
        "                selected_states = [all_hidden_states[idx] for idx in selected_indices]\n",
        "\n",
        "                layer_pooling = getattr(model.backbone, \"layer_pooling\", None)\n",
        "                if layer_pooling is None or not hasattr(layer_pooling, \"weights\"):\n",
        "                    raise RuntimeError(\"Backbone missing layer_pooling.weights needed for layer_contrib probing.\")\n",
        "\n",
        "                weights = torch.softmax(layer_pooling.weights.detach(), dim=0)\n",
        "                if weights.numel() != len(selected_states):\n",
        "                    raise RuntimeError(\n",
        "                        \"Layer weight count does not match selected states: \"\n",
        "                        f\"weights={weights.numel()} states={len(selected_states)}\"\n",
        "                    )\n",
        "\n",
        "                for local_idx, (layer_idx, hidden_state) in enumerate(zip(selected_indices, selected_states)):\n",
        "                    contribution = hidden_state * weights[local_idx]\n",
        "                    pooled = contribution.mean(dim=1).cpu().numpy()\n",
        "                    layer_reps.setdefault(int(layer_idx), []).append(pooled)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Unknown representation '{representation}'. \"\n",
        "                    \"Expected one of: hidden_states, mixed, repr, layer_contrib\"\n",
        "                )\n",
        "\n",
        "            # Defensive access - y_codec/y_codec_q may not be present in all eval modes\n",
        "            y_codec = batch.get(\"y_codec\")\n",
        "            y_codec_q = batch.get(\"y_codec_q\")\n",
        "            if y_codec is not None:\n",
        "                all_codec.append(y_codec.numpy())\n",
        "            if y_codec_q is not None:\n",
        "                all_codec_q.append(y_codec_q.numpy())\n",
        "\n",
        "            n_samples += batch_size\n",
        "\n",
        "    for key in list(layer_reps.keys()):\n",
        "        layer_reps[key] = np.concatenate(layer_reps[key], axis=0)\n",
        "        if max_samples:\n",
        "            layer_reps[key] = layer_reps[key][:max_samples]\n",
        "\n",
        "    if not all_codec:\n",
        "        raise ValueError(\"No y_codec labels were found in batches. Cannot run codec probing.\")\n",
        "    if not all_codec_q:\n",
        "        raise ValueError(\"No y_codec_q labels were found in batches. Cannot run codec_q probing.\")\n",
        "\n",
        "    all_codec = np.concatenate(all_codec)\n",
        "    all_codec_q = np.concatenate(all_codec_q)\n",
        "    if max_samples:\n",
        "        all_codec = all_codec[:max_samples]\n",
        "        all_codec_q = all_codec_q[:max_samples]\n",
        "\n",
        "    return layer_reps, all_codec, all_codec_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select representation for domain probing.\n",
        "# Keep hidden_states to preserve layer-wise leakage profile.\n",
        "probe_representation = \"hidden_states\"\n",
        "\n",
        "# Create dataloader for probing\n",
        "probe_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=erm_config,\n",
        "    max_samples=5000,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "# Extract model A representations for probing\n",
        "print(f\"Extracting {model_a_name} representations for probing ({probe_representation})...\")\n",
        "erm_reps_probe, codec_labels, codec_q_labels = extract_representations_for_probing(\n",
        "    erm_model,\n",
        "    probe_loader,\n",
        "    device,\n",
        "    max_samples=5000,\n",
        "    representation=probe_representation,\n",
        ")\n",
        "print(f\"Extracted representation keys: {list(erm_reps_probe.keys())[:10]}\")\n",
        "print(f\"Codec labels shape: {codec_labels.shape}, unique values: {np.unique(codec_labels).shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run domain probes on model A\n",
        "print(f\"\\nRunning CODEC probes on {model_a_name}...\")\n",
        "erm_probe_inputs = {k: v for k, v in erm_reps_probe.items() if isinstance(k, int)}\n",
        "if not erm_probe_inputs:\n",
        "    raise RuntimeError(\n",
        "        \"Layer-wise probing requires integer layer keys. \"\n",
        "        f\"Current probe_representation={probe_representation} produced keys={list(erm_reps_probe.keys())}\"\n",
        "    )\n",
        "\n",
        "erm_codec_probes = layerwise_probing(\n",
        "    erm_probe_inputs,\n",
        "    codec_labels,\n",
        "    classifier=\"logistic\",\n",
        "    cv_folds=5,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "print(f\"{model_a_name} max leakage layer: {erm_codec_probes['max_leakage_layer']}\")\n",
        "print(f\"{model_a_name} max accuracy: {erm_codec_probes['max_leakage_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate dataloader and extract patched model representations\n",
        "probe_loader = create_eval_dataloader(\n",
        "    split=\"dev\",\n",
        "    codec_vocab=codec_vocab,\n",
        "    codec_q_vocab=codec_q_vocab,\n",
        "    config=erm_config,\n",
        "    max_samples=5000,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "print(f\"Extracting patched model representations for probing ({probe_representation})...\")\n",
        "patched_reps_probe, _, _ = extract_representations_for_probing(\n",
        "    patched_model,\n",
        "    probe_loader,\n",
        "    device,\n",
        "    max_samples=5000,\n",
        "    representation=probe_representation,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run domain probes on patched model\n",
        "print(\"Running CODEC probes on patched model...\")\n",
        "patched_probe_inputs = {k: v for k, v in patched_reps_probe.items() if isinstance(k, int)}\n",
        "if not patched_probe_inputs:\n",
        "    raise RuntimeError(\n",
        "        \"Layer-wise probing requires integer layer keys. \"\n",
        "        f\"Current probe_representation={probe_representation} produced keys={list(patched_reps_probe.keys())}\"\n",
        "    )\n",
        "\n",
        "patched_codec_probes = layerwise_probing(\n",
        "    patched_probe_inputs,\n",
        "    codec_labels,\n",
        "    classifier=\"logistic\",\n",
        "    cv_folds=5,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "print(f\"Patched Max leakage layer: {patched_codec_probes['max_leakage_layer']}\")\n",
        "print(f\"Patched Max accuracy: {patched_codec_probes['max_leakage_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DOMAIN PROBE COMPARISON (CODEC)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Layer':<10} {f'{model_a_name} Acc':<18} {f'{patched_model_name} Acc':<22} {'Reduction':<12} {'Patched?':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for layer in sorted([k for k in erm_codec_probes['per_layer'].keys()]):\n",
        "    erm_result = erm_codec_probes['per_layer'][layer]\n",
        "    patched_result = patched_codec_probes['per_layer'][layer]\n",
        "\n",
        "    erm_acc = erm_result.get('accuracy', float('nan'))\n",
        "    patched_acc = patched_result.get('accuracy', float('nan'))\n",
        "\n",
        "    if np.isfinite(erm_acc) and np.isfinite(patched_acc):\n",
        "        reduction = erm_acc - patched_acc\n",
        "        is_patched = \"✓\" if layer in divergent_layers else \"\"\n",
        "        print(f\"{layer:<10} {erm_acc:<18.4f} {patched_acc:<22.4f} {reduction:+12.4f} {is_patched:<10}\")\n",
        "    else:\n",
        "        print(f\"{layer:<10} {'skipped':<18} {'skipped':<22} {'-':<12}\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot probe accuracy comparison\n",
        "layers_for_plot = sorted([k for k in erm_codec_probes['per_layer'].keys() if isinstance(k, int)])\n",
        "erm_accs = []\n",
        "patched_accs = []\n",
        "\n",
        "for layer in layers_for_plot:\n",
        "    erm_acc = erm_codec_probes['per_layer'][layer].get('accuracy', float('nan'))\n",
        "    patched_acc = patched_codec_probes['per_layer'][layer].get('accuracy', float('nan'))\n",
        "    erm_accs.append(erm_acc if np.isfinite(erm_acc) else 0)\n",
        "    patched_accs.append(patched_acc if np.isfinite(patched_acc) else 0)\n",
        "\n",
        "x = np.arange(len(layers_for_plot))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "bars1 = ax.bar(x - width/2, erm_accs, width, label=model_a_name, color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, patched_accs, width, label=patched_model_name, color='coral')\n",
        "\n",
        "# Highlight patched layers\n",
        "for i, layer in enumerate(layers_for_plot):\n",
        "    if layer in divergent_layers:\n",
        "        ax.axvspan(i - 0.5, i + 0.5, alpha=0.2, color='yellow')\n",
        "\n",
        "ax.set_xlabel('Layer', fontsize=12)\n",
        "ax.set_ylabel('Domain Probe Accuracy', fontsize=12)\n",
        "ax.set_title(\n",
        "    f'Domain Probe Accuracy: {model_a_name} vs {patched_model_name}\\n'\n",
        "    '(Yellow = Patched Layers, Lower = More Domain Invariant)',\n",
        "    fontsize=14,\n",
        ")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(layers_for_plot)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.axhline(y=1/len(np.unique(codec_labels)), color='gray', linestyle='--', alpha=0.5, label='Chance level')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": model_a_name,\n",
        "        \"Eval EER\": f\"{erm_results['eer']:.2%}\",\n",
        "        \"Min DCF\": f\"{erm_results['min_dcf']:.4f}\",\n",
        "        \"Max Codec Probe Acc\": f\"{erm_codec_probes['max_leakage_accuracy']:.2%}\",\n",
        "        \"Max Leakage Layer\": erm_codec_probes['max_leakage_layer'],\n",
        "        \"Notes\": \"Base model\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": model_b_name,\n",
        "        \"Eval EER\": f\"{dann_results['eer']:.2%}\",\n",
        "        \"Min DCF\": f\"{dann_results['min_dcf']:.4f}\",\n",
        "        \"Max Codec Probe Acc\": \"n/a\",\n",
        "        \"Max Leakage Layer\": \"n/a\",\n",
        "        \"Notes\": \"Donor model\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": patched_model_name,\n",
        "        \"Eval EER\": f\"{patched_results['eer']:.2%}\",\n",
        "        \"Min DCF\": f\"{patched_results['min_dcf']:.4f}\",\n",
        "        \"Max Codec Probe Acc\": f\"{patched_codec_probes['max_leakage_accuracy']:.2%}\",\n",
        "        \"Max Leakage Layer\": patched_codec_probes['max_leakage_layer'],\n",
        "        \"Notes\": f\"Layers {divergent_layers} patched from {model_b_name}\"\n",
        "    },\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*100)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to file\n",
        "results_df.to_csv(\"rq4_results_summary.csv\", index=False)\n",
        "print(\"Results saved to rq4_results_summary.csv\")\n",
        "\n",
        "# Save CKA results\n",
        "cka_df = pd.DataFrame([\n",
        "    {\"Layer\": layer, \"CKA\": cka_results['per_layer'][layer]['cka']}\n",
        "    for layer in sorted(cka_results['per_layer'].keys())\n",
        "])\n",
        "cka_df.to_csv(\"rq4_cka_results.csv\", index=False)\n",
        "print(\"CKA results saved to rq4_cka_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **CKA Analysis**\n",
        "   - Compared model pair: `model_a_name` vs `model_b_name`\n",
        "   - Representation mode for CKA: `cka_representation`\n",
        "   - Most divergent patchable layers: `divergent_layers`\n",
        "\n",
        "2. **Performance Impact**\n",
        "   - Base model metric: `erm_results`\n",
        "   - Donor model metric: `dann_results`\n",
        "   - Patched model metric: `patched_results`\n",
        "\n",
        "3. **Domain Invariance**\n",
        "   - Base model max probe leakage: `erm_codec_probes['max_leakage_accuracy']`\n",
        "   - Patched model max probe leakage: `patched_codec_probes['max_leakage_accuracy']`\n",
        "\n",
        "### Scope Notes\n",
        "\n",
        "- This notebook currently supports checkpoints whose `training.method` is `erm` or `dann`.\n",
        "- In this codebase, DANN applies the domain objective to pre-projection pooled features.\n",
        "- With frozen backbones, differences are expected mostly in post-backbone representations.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "- **Computational cost:** Patching runs donor + base during inference (roughly 2x forward passes).\n",
        "- **Flexibility:** You can target specific layers without retraining.\n",
        "- **Simplicity:** Useful as a lightweight intervention once donor checkpoints exist.\n",
        "\n",
        "### Future Work\n",
        "\n",
        "1. **Selective patching:** Patch only samples likely to benefit.\n",
        "2. **Partial patching:** Interpolate between base and donor activations.\n",
        "3. **Distillation:** Train one model to mimic patched behavior.\n",
        "4. **Multi-layer analysis:** Study interactions across patch sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "if 'patched_model' in globals() and patched_model is not None:\n",
        "    patched_model.remove_hooks()\n",
        "    print(\"Hooks cleaned up.\")\n",
        "else:\n",
        "    print(\"No patched model found; skipping hook cleanup.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
