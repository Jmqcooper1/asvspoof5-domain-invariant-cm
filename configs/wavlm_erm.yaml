# WavLM Base+ ERM Configuration
# Single-file config for ERM training with WavLM backbone

# Backbone
backbone:
  name: wavlm_base_plus
  pretrained: microsoft/wavlm-base-plus
  num_layers: 12
  hidden_size: 768
  freeze: true

  layer_selection:
    method: weighted
    k: 6
    init_lower_bias: true

# Pooling
pooling:
  method: stats  # mean+std concatenation

# Projection head
projection:
  input_dim: 1536  # 768 * 2 for stats pooling
  hidden_dim: 512
  output_dim: 256
  num_layers: 2
  dropout: 0.1

# Task classifier
classifier:
  input_dim: 256
  num_classes: 2
  hidden_dim: 0  # Direct linear
  dropout: 0.1

# Training
training:
  method: erm

  optimizer:
    name: adamw
    lr: 1.0e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]

  scheduler:
    name: cosine
    warmup_steps: 500
    min_lr: 1.0e-6

  max_epochs: 50
  patience: 10
  min_delta: 0.001  # Minimum improvement to count as progress (for EER)
  train_loss_threshold: 0.0  # Disable train-loss early stopping for comparability
  plateau_patience: 3  # Consecutive low train loss epochs before early stop
  gradient_clip: 1.0

  save_every_n_epochs: 5
  save_best_only: true
  monitor_metric: eer
  monitor_mode: min

# Loss
loss:
  task:
    name: cross_entropy
    weight: 1.0
    label_smoothing: 0.0

# Audio
audio:
  sample_rate: 16000
  max_duration_sec: 6.0

# Dataloader
dataloader:
  batch_size: 32
  num_workers: 4
  pin_memory: true
  drop_last: true

# Logging
logging:
  log_every_n_steps: 50
  val_every_n_epochs: 1
  log_domain_breakdown_every: 5  # Per-domain metrics frequency
  json_logs: true  # Write JSON logs to file
  log_batch_samples: 0.02  # Sample 2% of batches for detailed logging
  track_gradients: true  # Log gradient norms
  track_layer_weights: true  # Log layer mixing weights

# Wandb
wandb:
  enabled: true
  project: asvspoof5-dann
  entity: null
  tags: ["wavlm", "erm", "track1"]
  log_model: false  # Save model as artifact
  log_code: true  # Log code snapshot

# Reproducibility
seed: 42
deterministic: true
