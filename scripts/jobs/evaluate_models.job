#!/bin/bash
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=Evaluate
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=04:00:00
#SBATCH --output=./scripts/jobs/out/%x_%A.out

# =============================================================================
# Evaluate All Trained Models
# =============================================================================

module purge
module load 2025

# Navigate to repo root (works regardless of where repo is cloned)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR/../.."

# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh
export PATH="$HOME/.local/bin:$PATH"
uv sync --locked

# Verify CUDA availability
uv run python - <<'EOF'
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU device count: {torch.cuda.device_count()}")
    print(f"Active GPU: {torch.cuda.get_device_name(0)}")
EOF
echo ""

# Load env + require ASVSPOOF5_ROOT
# shellcheck disable=SC1091
source scripts/jobs/_job_common.sh
load_and_require_asvspoof5_root

# Optional defaults
export WANDB_PROJECT=${WANDB_PROJECT:-asvspoof5-dann}

echo "=== Evaluating All Models ==="

# Evaluate WavLM ERM
echo ""
echo "--- WavLM ERM ---"
srun uv run python scripts/evaluate.py \
    --checkpoint runs/wavlm_erm/checkpoints/best.pt \
    --split dev \
    --wandb

# Evaluate WavLM DANN
echo ""
echo "--- WavLM DANN ---"
srun uv run python scripts/evaluate.py \
    --checkpoint runs/wavlm_dann/checkpoints/best.pt \
    --split dev \
    --wandb

# Evaluate Wav2Vec2 ERM
echo ""
echo "--- Wav2Vec2 ERM ---"
srun uv run python scripts/evaluate.py \
    --checkpoint runs/w2v2_erm/checkpoints/best.pt \
    --split dev \
    --wandb

# Evaluate Wav2Vec2 DANN
echo ""
echo "--- Wav2Vec2 DANN ---"
srun uv run python scripts/evaluate.py \
    --checkpoint runs/w2v2_dann/checkpoints/best.pt \
    --split dev \
    --wandb

echo ""
echo "Evaluation complete!"
