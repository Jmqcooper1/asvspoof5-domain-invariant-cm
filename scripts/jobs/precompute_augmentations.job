#!/bin/bash
#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=precompute_aug
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=08:00:00
#SBATCH --array=0-14
#SBATCH --output=./scripts/jobs/out/%x_%A_%a.out

# =============================================================================
# Pre-compute Codec Augmentations for DANN Training (SLURM Job Array)
# =============================================================================
#
# Run this BEFORE training jobs. It pre-computes all codec × quality
# augmentations offline so DANN training doesn't need on-the-fly ffmpeg.
#
# Uses a SLURM job array (15 shards) to parallelize across ~1.8M files.
# Each shard processes ~122k files with 9 workers on a gpu_mig slice (cheapest GPU partition).
# Expected runtime: ~4h per shard (8h wall-clock limit for safety margin).
# Expected disk usage: ~5-10 GB for train split (15 augmentations/file).
#
# Resume support: re-running skips already-completed files, so it's safe to
# resubmit if any shard fails or times out.
#
# Submit from repo root (2 steps):
#   1. JOB_ID=$(sbatch --parsable scripts/jobs/precompute_augmentations.job)
#   2. sbatch --dependency=afterok:$JOB_ID scripts/jobs/merge_augmentation_manifests.job
#
# Or simply:
#   sbatch scripts/jobs/precompute_augmentations.job
#   # After all shards finish, run the merge:
#   uv run python scripts/precompute_augmentations.py --output-dir /scratch-shared/$USER/asvspoof5_augmented_cache --merge-manifests
# =============================================================================

module purge
module load 2025

# Navigate to repo root
if [[ -n "${SLURM_SUBMIT_DIR:-}" ]]; then
  cd "$SLURM_SUBMIT_DIR"
fi
if [[ ! -f "pyproject.toml" ]]; then
  echo "ERROR: pyproject.toml not found in $(pwd). Submit from repo root." >&2
  exit 1
fi
echo "Repo root: $(pwd)"

# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh
export PATH="$HOME/.local/bin:$PATH"
uv sync --locked

# Load env + require ASVSPOOF5_ROOT
# shellcheck disable=SC1091
source scripts/jobs/_job_common.sh
load_dotenv_if_present
require_env_var "ASVSPOOF5_ROOT"
print_env_diagnostics

# Load FFmpeg (required for codec encoding/decoding)
load_ffmpeg_module
check_ffmpeg_encoders_or_fail

# Output directory — use scratch for fast I/O
OUTPUT_DIR="${AUGMENTATION_CACHE_DIR:-/scratch-shared/$USER/asvspoof5_augmented_cache}"

echo ""
echo "=== Pre-compute Codec Augmentations (Shard ${SLURM_ARRAY_TASK_ID}/15) ==="
echo "Output: $OUTPUT_DIR"
echo "Workers: ${SLURM_CPUS_PER_TASK}"
echo ""

srun uv run python scripts/precompute_augmentations.py \
    --data-root "$ASVSPOOF5_ROOT" \
    --output-dir "$OUTPUT_DIR" \
    --codecs MP3 AAC OPUS \
    --qualities 1 2 3 4 5 \
    --num-workers "${SLURM_CPUS_PER_TASK}" \
    --split train \
    --shard-index "${SLURM_ARRAY_TASK_ID}" \
    --num-shards 15

echo ""
echo "=== Done (Shard ${SLURM_ARRAY_TASK_ID}/15) ==="
echo "Cache directory: $OUTPUT_DIR"
echo ""
echo "To use with training, set in your config or .env:"
echo "  augmentation.cache_dir: $OUTPUT_DIR"
echo ""
echo "Or add to .env:"
echo "  AUGMENTATION_CACHE_DIR=$OUTPUT_DIR"
